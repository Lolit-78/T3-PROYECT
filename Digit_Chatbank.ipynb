{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyM6hX6hCzOarG7xPLp9rd3S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lolit-78/T3-PROYECT/blob/main/Digit_Chatbank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHATBOT MULTIMODAL"
      ],
      "metadata": {
        "id": "qOT94obqQmmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENTRENAMIENTO DE MODELO DE CLASIFICACIÓN DE INTENSIONES BANCARIAS"
      ],
      "metadata": {
        "id": "Li4MgHXkQ0j5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TwAELIfCU7J"
      },
      "outputs": [],
      "source": [
        "# === 1. Instalar dependencias (solo si no las tienes) ===\n",
        "# !pip install scikit-learn pandas numpy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "# === 2. Cargar dataset y parsear columnas ===\n",
        "# Leer el archivo como una sola columna inicialmente\n",
        "try:\n",
        "    df_raw = pd.read_csv(\"dataset_bancario_espanol.csv\", encoding='utf-8', header=None)\n",
        "except UnicodeDecodeError:\n",
        "    try:\n",
        "        df_raw = pd.read_csv(\"dataset_bancario_espanol.csv\", encoding='latin1', header=None)\n",
        "    except UnicodeDecodeError:\n",
        "        df_raw = pd.read_csv(\"dataset_bancario_espanol.csv\", encoding='ISO-8859-1', header=None)\n",
        "\n",
        "# Expresión regular para extraer el texto y la intención\n",
        "# Busca cualquier cosa hasta una coma, seguida de comillas, cualquier cosa (la intención) y comillas\n",
        "pattern = re.compile(r'([^,]*),\"(.*)\"')\n",
        "\n",
        "# Aplicar la expresión regular para crear las columnas 'texto' e 'intencion'\n",
        "def parse_row(row):\n",
        "    match = pattern.match(row[0])\n",
        "    if match:\n",
        "        return pd.Series([match.group(1), match.group(2)])\n",
        "    return pd.Series([None, None]) # Retorna None si no coincide el patrón\n",
        "\n",
        "df = df_raw.apply(parse_row, axis=1)\n",
        "df.columns = ['texto', 'intencion']\n",
        "\n",
        "# Eliminar filas donde no se pudo extraer texto o intención\n",
        "df.dropna(subset=['texto', 'intencion'], inplace=True)\n",
        "\n",
        "\n",
        "print(\"Shape of dataframe after parsing:\", df.shape)\n",
        "display(df.head()) # Mostrar las primeras filas para verificar\n",
        "\n",
        "# === 3. Preprocesamiento básico ===\n",
        "def limpiar_texto(texto):\n",
        "    # Asegurarse de que 'texto' sea una cadena antes de aplicar lower()\n",
        "    if isinstance(texto, str):\n",
        "        texto = texto.lower()\n",
        "        texto = re.sub(r\"[^a-záéíóúñ0-9\\s]\", \"\", texto)\n",
        "        return texto\n",
        "    return \"\" # Retorna una cadena vacía si no es una cadena\n",
        "\n",
        "\n",
        "df[\"texto\"] = df[\"texto\"].apply(limpiar_texto)\n",
        "\n",
        "\n",
        "# Check if dataframe is empty after dropping rows\n",
        "if df.empty:\n",
        "    print(\"The dataframe is empty after cleaning and dropping rows.\")\n",
        "else:\n",
        "    # === 4. Vectorización de texto ===\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(df[\"texto\"]).toarray()\n",
        "\n",
        "    # === 5. Codificación de etiquetas (one-hot) ===\n",
        "    encoder = OneHotEncoder()\n",
        "    y = encoder.fit_transform(df[[\"intencion\"]]).toarray()\n",
        "\n",
        "    # === 6. División en train/test ===\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # === 7. Clase NeuralNetwork (desde cero con NumPy) ===\n",
        "    def relu(x): return np.maximum(0, x)\n",
        "    def relu_derivative(x): return (x > 0).astype(float)\n",
        "    def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
        "    def tanh(x): return np.tanh(x) # Add tanh activation\n",
        "    def tanh_derivative(x): return 1.0 - np.tanh(x)**2 # Add tanh derivative\n",
        "\n",
        "    def xavier_init(size_in, size_out, activation='relu'):\n",
        "        if activation == 'relu':\n",
        "            std = np.sqrt(2.0 / size_in)\n",
        "        elif activation == 'tanh': # Handle tanh initialization\n",
        "             std = np.sqrt(2.0 / (size_in + size_out)) # Or other tanh-specific init if preferred\n",
        "        else: # Default for sigmoid or others\n",
        "            std = np.sqrt(1.0 / size_in)\n",
        "        return np.random.randn(size_in, size_out) * std\n",
        "\n",
        "    class NeuralNetwork:\n",
        "        def __init__(self, layers, activation='relu', seed=42):\n",
        "            np.random.seed(seed)\n",
        "            self.layers = layers\n",
        "            self.activation_name = activation\n",
        "            self.params = {}\n",
        "            self.L = len(layers) - 1\n",
        "            self.loss_history = [] # Add list to store loss history\n",
        "            for i in range(1, len(layers)):\n",
        "                act = activation if i < len(layers)-1 else 'sigmoid'\n",
        "                self.params[f'W{i}'] = xavier_init(layers[i-1], layers[i], activation=act)\n",
        "                self.params[f'b{i}'] = np.zeros((1, layers[i]))\n",
        "\n",
        "        def _activation(self, x):\n",
        "            if self.activation_name == 'relu':\n",
        "                return relu(x)\n",
        "            elif self.activation_name == 'sigmoid':\n",
        "                return sigmoid(x)\n",
        "            elif self.activation_name == 'tanh':\n",
        "                 return tanh(x)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "        def _activation_derivative(self, x):\n",
        "            if self.activation_name == 'relu':\n",
        "                return relu_derivative(x)\n",
        "            elif self.activation_name == 'sigmoid':\n",
        "                return sigmoid(x) * (1 - sigmoid(x))\n",
        "            elif self.activation_name == 'tanh':\n",
        "                return tanh_derivative(x)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "        def forward(self, X):\n",
        "            caches = {'A0': X}\n",
        "            A = X\n",
        "            for i in range(1, self.L + 1):\n",
        "                W, b = self.params[f'W{i}'], self.params[f'b{i}']\n",
        "                Z = np.dot(A, W) + b\n",
        "                caches[f'Z{i}'] = Z\n",
        "                if i == self.L:\n",
        "                    # Softmax for the output layer\n",
        "                    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "                    A = expZ / np.sum(expZ, axis=1, keepdims=True)\n",
        "                else:\n",
        "                    A = self._activation(Z)\n",
        "                caches[f'A{i}'] = A\n",
        "            return A, caches\n",
        "\n",
        "        def compute_loss(self, Y_hat, Y):\n",
        "            m = Y.shape[0]\n",
        "            eps = 1e-9\n",
        "            # Using Categorical Cross-Entropy loss\n",
        "            loss = -np.sum(Y * np.log(Y_hat + eps)) / m\n",
        "            return loss\n",
        "\n",
        "\n",
        "        def backward(self, caches, X, Y):\n",
        "            grads = {}\n",
        "            m = X.shape[0]\n",
        "            # Gradient for the output layer (Softmax + Cross-Entropy)\n",
        "            dZ = (caches[f'A{self.L}'] - Y) / m\n",
        "\n",
        "            for i in range(self.L, 0, -1):\n",
        "                A_prev = caches[f'A{i-1}']\n",
        "                grads[f'dW{i}'] = np.dot(A_prev.T, dZ)\n",
        "                grads[f'db{i}'] = np.sum(dZ, axis=0, keepdims=True)\n",
        "\n",
        "                if i > 1:\n",
        "                    # Backpropagate through the activation function of the previous layer\n",
        "                    W = self.params[f'W{i}']\n",
        "                    dA_prev = np.dot(dZ, W.T)\n",
        "                    dZ = dA_prev * self._activation_derivative(caches[f'Z{i-1}'])\n",
        "            return grads\n",
        "\n",
        "\n",
        "        def update_params(self, grads, lr):\n",
        "            for i in range(1, self.L + 1): # Corrected loop range\n",
        "                self.params[f'W{i}'] -= lr * grads[f'dW{i}']\n",
        "                self.params[f'b{i}'] -= lr * grads[f'db{i}']\n",
        "\n",
        "\n",
        "        def fit(self, X, Y, epochs=100, lr=0.01):\n",
        "            self.loss_history = [] # Reset loss history at the start of fitting\n",
        "            for epoch in range(epochs):\n",
        "                Y_hat, caches = self.forward(X)\n",
        "                loss = self.compute_loss(Y_hat, Y)\n",
        "                self.loss_history.append(loss) # Store the recorded loss history\n",
        "                grads = self.backward(caches, X, Y)\n",
        "                self.update_params(grads, lr)\n",
        "                if (epoch + 1) % 10 == 0 or epoch == 1:\n",
        "                    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
        "            return self.loss_history # Return loss history\n",
        "\n",
        "\n",
        "        def predict(self, X):\n",
        "            Y_hat, _ = self.forward(X)\n",
        "            # Return the index of the class with the highest probability\n",
        "            return np.argmax(Y_hat, axis=1)\n",
        "\n",
        "    # === 8. Entrenar la red ===\n",
        "    n_features = X_train.shape[1]\n",
        "    n_classes = y_train.shape[1]\n",
        "    nn = NeuralNetwork([n_features, 64, 32, n_classes], activation='relu')\n",
        "    nn.fit(X_train, y_train, epochs=100, lr=0.01)\n",
        "\n",
        "    # === 9. Evaluación ===\n",
        "    y_pred = nn.predict(X_test)\n",
        "    y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Get the unique labels present in the test set\n",
        "    unique_test_labels = np.unique(y_test_labels)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(y_test_labels, y_pred))\n",
        "    # Pass the unique labels from the test set to the labels parameter\n",
        "    print(classification_report(y_test_labels, y_pred, target_names=encoder.categories_[0], labels=unique_test_labels))\n",
        "    print(\"Matriz de confusión:\\n\", confusion_matrix(y_test_labels, y_pred))\n",
        "\n",
        "    # === 10. Guardar modelo y preprocesadores ===\n",
        "    with open(\"vectorizer.pkl\", \"wb\") as f: pickle.dump(vectorizer, f)\n",
        "    with open(\"encoder.pkl\", \"wb\") as f: pickle.dump(encoder, f)\n",
        "    with open(\"nn_weights.pkl\", \"wb\") as f: pickle.dump(nn.params, f)"
      ]
    },
    {
      "source": [
        "from google.colab import sheets\n",
        "sheet = sheets.InteractiveSheet(df=df)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "GCguUF1ZD0mr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9f0a3eb"
      },
      "source": [
        "### Predicciones con el modelo entrenado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "153bdb4d"
      },
      "source": [
        "# === Ensure files are saved before loading (for diagnosis) ===\n",
        "# Check if dataframe is empty before attempting to save\n",
        "if 'df' in locals() and not df.empty:\n",
        "    try:\n",
        "        with open(\"vectorizer.pkl\", \"wb\") as f: pickle.dump(vectorizer, f)\n",
        "        with open(\"encoder.pkl\", \"wb\") as f: pickle.dump(encoder, f)\n",
        "        with open(\"nn_weights.pkl\", \"wb\") as f: pickle.dump(nn.params, f)\n",
        "        print(\"Model and preprocessor files saved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving files: {e}\")\n",
        "else:\n",
        "    print(\"DataFrame is empty. Skipping file saving.\")\n",
        "\n",
        "# === 1. Cargar modelo y preprocesadores ===\n",
        "import os\n",
        "print(\"Files in current directory:\", os.listdir())\n",
        "\n",
        "try:\n",
        "    with open(\"vectorizer.pkl\", \"rb\") as f:\n",
        "        loaded_vectorizer = pickle.load(f)\n",
        "    with open(\"encoder.pkl\", \"rb\") as f:\n",
        "        loaded_encoder = pickle.load(f)\n",
        "    with open(\"nn_weights.pkl\", \"rb\") as f:\n",
        "        loaded_weights = pickle.load(f)\n",
        "    print(\"Model and preprocessor files loaded successfully.\")\n",
        "\n",
        "    # Recreate the neural network structure and load weights\n",
        "    # Get vocabulary size from the loaded vectorizer safely\n",
        "    n_features = len(loaded_vectorizer.vocabulary_) if hasattr(loaded_vectorizer, 'vocabulary_') else 0\n",
        "    # Get number of classes from the loaded encoder safely\n",
        "    n_classes = loaded_encoder.categories_[0].shape[0] if hasattr(loaded_encoder, 'categories_') and loaded_encoder.categories_ else 0\n",
        "\n",
        "    if n_features > 0 and n_classes > 0:\n",
        "        loaded_nn = NeuralNetwork([n_features, 64, 32, n_classes], activation='relu')\n",
        "        loaded_nn.params = loaded_weights\n",
        "\n",
        "        # === 2. Preprocess new text ===\n",
        "        new_text = [\"Quiero saber mi saldo actual\", \"Necesito información sobre un préstamo\"]\n",
        "        cleaned_text = [limpiar_texto(text) for text in new_text]\n",
        "\n",
        "        # === 3. Vectorize new text ===\n",
        "        X_new = loaded_vectorizer.transform(cleaned_text).toarray()\n",
        "\n",
        "        # === 4. Predict intention ===\n",
        "        predictions = loaded_nn.predict(X_new)\n",
        "\n",
        "        # === 5. Decode predictions ===\n",
        "        predicted_intents = loaded_encoder.categories_[0][predictions]\n",
        "\n",
        "        # === 6. Display results ===\n",
        "        for text, intent in zip(new_text, predicted_intents):\n",
        "            print(f\"Text: {text} -> Predicted Intent: {intent}\")\n",
        "    else:\n",
        "        print(\"Unable to predict: vocabulary or number of classes is zero.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Model or preprocessor files not found after attempting to save in this cell.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during loading or prediction: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENTRENAMIENTO CON OCR E IMAGENES DE DOCUMENTOS BANCARIOS SIMULADOS"
      ],
      "metadata": {
        "id": "aVTwudIPQVp0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d6fc855"
      },
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Create a directory to save simulated images\n",
        "if not os.path.exists(\"simulated_docs\"):\n",
        "    os.makedirs(\"simulated_docs\")\n",
        "\n",
        "def create_simulated_document(text, filename, img_size=(600, 200), bg_color=\"white\", text_color=\"black\"):\n",
        "    \"\"\"Creates a basic simulated document image with text.\"\"\"\n",
        "    img = Image.new('RGB', img_size, color = bg_color)\n",
        "    d = ImageDraw.Draw(img)\n",
        "\n",
        "    try:\n",
        "        # Try to use a common font available in Colab, or a default\n",
        "        font = ImageFont.truetype(\"LiberationSerif-Regular.ttf\", 25)\n",
        "    except IOError:\n",
        "        print(\"Default font not found, using a generic PIL font.\")\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    # Add text wrapping for longer sentences\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    current_line = \"\"\n",
        "    for word in words:\n",
        "        # Estimate text width (very basic)\n",
        "        test_line = current_line + word + \" \"\n",
        "        if font.getbbox(test_line)[2] < img_size[0] - 40: # Check if line exceeds width (with padding)\n",
        "            current_line += word + \" \"\n",
        "        else:\n",
        "            lines.append(current_line.strip())\n",
        "            current_line = word + \" \"\n",
        "    lines.append(current_line.strip())\n",
        "\n",
        "    y_text = 20\n",
        "    for line in lines:\n",
        "        d.text((20, y_text), line, fill=text_color, font=font)\n",
        "        y_text += font.getbbox(line)[3] - font.getbbox(line)[1] + 5 # Move down for next line\n",
        "\n",
        "    img.save(os.path.join(\"simulated_docs\", filename))\n",
        "    print(f\"Created simulated document: {filename}\")\n",
        "\n",
        "# Define some banking-related texts\n",
        "banking_texts = [\n",
        "    \"Mi saldo actual es de 1500 dolares.\",\n",
        "    \"Quiero transferir 500 a Juan Perez.\",\n",
        "    \"Necesito información sobre un préstamo personal.\",\n",
        "    \"Pagar mi tarjeta de crédito Visa.\",\n",
        "    \"Consultar los movimientos de mi cuenta de ahorros.\",\n",
        "    \"Quiero abrir una nueva cuenta corriente.\"\n",
        "]\n",
        "\n",
        "# Create images for each text\n",
        "for i, text in enumerate(banking_texts):\n",
        "    create_simulated_document(text, f\"document_{i+1}.png\")\n",
        "\n",
        "print(\"\\nSimulated documents created in 'simulated_docs' directory.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eaa4974"
      },
      "source": [
        "# === 1. Instalar dependencias para OCR e imágenes ===\n",
        "!apt-get update\n",
        "!apt-get install tesseract-ocr tesseract-ocr-spa\n",
        "!pip install pytesseract Pillow opencv-python\n",
        "\n",
        "import pytesseract\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0609e0a8"
      },
      "source": [
        "# === 3. Preprocesamiento de imágenes ===\n",
        "def preprocess_image_for_ocr(image_path):\n",
        "    \"\"\"Loads an image and applies basic preprocessing for OCR.\"\"\"\n",
        "    # Load the image using OpenCV\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    if img is None:\n",
        "        print(f\"Error: Could not load image from {image_path}\")\n",
        "        return None\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply thresholding to get a binary image\n",
        "    # Adjust the thresholding method and parameters as needed\n",
        "    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "\n",
        "    # Optional: Denoising, if needed\n",
        "    # binary = cv2.fastNlMeansDenoising(binary, None, 30, 7, 21)\n",
        "\n",
        "    # Optional: Skew correction (more advanced)\n",
        "\n",
        "    return binary\n",
        "\n",
        "# Example: Preprocess one of the generated images\n",
        "# Get the list of simulated document images\n",
        "image_files = [f for f in os.listdir(\"simulated_docs\") if f.endswith(\".png\")]\n",
        "\n",
        "if image_files:\n",
        "    sample_image_path = os.path.join(\"simulated_docs\", image_files[0])\n",
        "    preprocessed_img = preprocess_image_for_ocr(sample_image_path)\n",
        "\n",
        "    if preprocessed_img is not None:\n",
        "        print(f\"Image '{image_files[0]}' preprocessed successfully. Displaying result:\")\n",
        "        # Display the preprocessed image (optional, requires matplotlib or similar)\n",
        "        # For simple verification, we can just confirm it was processed.\n",
        "        # To display in Colab, you might need:\n",
        "        # from google.colab.patches import cv2_imshow\n",
        "        # cv2_imshow(preprocessed_img)\n",
        "    else:\n",
        "        print(f\"Failed to preprocess image '{image_files[0]}'.\")\n",
        "else:\n",
        "    print(\"No simulated document images found in 'simulated_docs' directory.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64a5ec0e"
      },
      "source": [
        "# === 4. Extracción de texto con OCR ===\n",
        "def extract_text_from_image(image_path):\n",
        "    \"\"\"Extracts text from an image using Tesseract OCR after preprocessing.\"\"\"\n",
        "    # Preprocess the image\n",
        "    preprocessed_img = preprocess_image_for_ocr(image_path)\n",
        "\n",
        "    if preprocessed_img is None:\n",
        "        return \"Error: Could not preprocess image.\"\n",
        "\n",
        "    # Use pytesseract to extract text from the preprocessed image\n",
        "    # We need to convert the OpenCV image (NumPy array) to a PIL Image for pytesseract\n",
        "    text = pytesseract.image_to_string(Image.fromarray(preprocessed_img), lang='spa') # Specify Spanish language\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example: Extract text from one of the generated images\n",
        "if image_files:\n",
        "    sample_image_path = os.path.join(\"simulated_docs\", image_files[0])\n",
        "    extracted_text = extract_text_from_image(sample_image_path)\n",
        "\n",
        "    print(f\"Extracted text from '{image_files[0]}':\")\n",
        "    print(extracted_text)\n",
        "else:\n",
        "    print(\"No simulated document images found for OCR.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e540308a"
      },
      "source": [
        "# === 5. Limpieza y estructuración del texto extraído ===\n",
        "def clean_extracted_text(text):\n",
        "    \"\"\"Applies basic cleaning to text extracted by OCR.\"\"\"\n",
        "    # Ensure text is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    # Replace multiple whitespace characters with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove characters that might be misread by OCR but are not alphanumeric or common punctuation\n",
        "    # (Adjust regex based on expected text content and common OCR errors)\n",
        "    # For now, let's keep it simple and remove non-alphanumeric except spaces and some punctuation\n",
        "    # text = re.sub(r'[^a-zA-Z0-9\\s.,!?;]', '', text) # Example: remove most special chars\n",
        "\n",
        "    # Convert to lowercase (optional, depending on downstream model)\n",
        "    # text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example: Clean the text extracted from the sample image\n",
        "if 'extracted_text' in locals():\n",
        "    cleaned_extracted_text = clean_extracted_text(extracted_text)\n",
        "    print(f\"Original extracted text:\\n{extracted_text}\")\n",
        "    print(f\"Cleaned extracted text:\\n{cleaned_extracted_text}\")\n",
        "else:\n",
        "    print(\"No extracted text found. Please run the OCR extraction step first.\")\n",
        "\n",
        "# Note: For real-world scenarios, structuring the text might involve\n",
        "# identifying sections (e.g., account number, amount) based on layout analysis or keywords.\n",
        "# For this basic example, we'll treat the extracted text as a single block."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4da6f578"
      },
      "source": [
        "# === Step 7: Preparación del conjunto de datos para entrenamiento ===\n",
        "\n",
        "# Get the list of simulated document images\n",
        "image_files = sorted([f for f in os.listdir(\"simulated_docs\") if f.endswith(\".png\")])\n",
        "\n",
        "# Define the original banking texts/intentions used for generating images\n",
        "# This list must match the order in which the images were generated (document_1.png -> first text, etc.)\n",
        "original_banking_texts = [\n",
        "    \"Mi saldo actual es de 1500 pesos.\", # Corresponds to document_1.png\n",
        "    \"Quiero transferir 500 a Juan Perez.\", # Corresponds to document_2.png\n",
        "    \"Necesito información sobre un préstamo personal.\", # Corresponds to document_3.png\n",
        "    \"Pagar mi tarjeta de crédito Visa.\", # Corresponds to document_4.png\n",
        "    \"Consultar los movimientos de mi cuenta de ahorros.\", # Corresponds to document_5.png\n",
        "    \"Quiero abrir una nueva cuenta corriente.\" # Corresponds to document_6.png\n",
        "]\n",
        "\n",
        "# We need to map these texts to their corresponding intentions as used in the original training data\n",
        "# Assuming a mapping exists or can be derived from the original df or a separate source\n",
        "# For this example, let's manually define a simple mapping based on the original training intents\n",
        "# Correcting keys to match the output of limpiar_texto (removing the final dot)\n",
        "text_to_intent_mapping = {\n",
        "    \"mi saldo actual es de 1500 pesos\": \"Consultar saldo\",\n",
        "    \"quiero transferir 500 a juan perez\": \"Transferir dinero\",\n",
        "    \"necesito información sobre un préstamo personal\": \"Solicitar préstamo\",\n",
        "    \"pagar mi tarjeta de crédito visa\": \"Pagar tarjeta de crédito\",\n",
        "    \"consultar los movimientos de mi cuenta de ahorros\": \"Consultar movimientos\",\n",
        "    \"quiero abrir una nueva cuenta corriente\": \"Abrir cuenta\"\n",
        "}\n",
        "# Note: This mapping assumes the cleaned text will match these keys exactly.\n",
        "# In a real scenario, you might need more flexible matching or store intentions directly with images.\n",
        "\n",
        "extracted_data = []\n",
        "\n",
        "# Process each simulated image\n",
        "for i, image_file in enumerate(image_files):\n",
        "    image_path = os.path.join(\"simulated_docs\", image_file)\n",
        "\n",
        "    # Preprocess, extract text, and clean\n",
        "    extracted_text = extract_text_from_image(image_path)\n",
        "    cleaned_text = clean_extracted_text(extracted_text)\n",
        "\n",
        "    # Get the original text used to generate this image (for mapping to intent)\n",
        "    # Need to clean the original text too to match the cleaned extracted text for mapping\n",
        "    original_cleaned_text = limpiar_texto(original_banking_texts[i])\n",
        "\n",
        "\n",
        "    # Find the corresponding intent using the mapping\n",
        "    # Use the cleaned original text as the key for robustness\n",
        "    corresponding_intent = text_to_intent_mapping.get(original_cleaned_text, \"Unknown Intent\")\n",
        "\n",
        "    extracted_data.append({\"image_file\": image_file, \"extracted_text\": cleaned_text, \"original_text\": original_banking_texts[i], \"intent\": corresponding_intent})\n",
        "\n",
        "# Create a DataFrame\n",
        "df_simulated_ocr = pd.DataFrame(extracted_data)\n",
        "\n",
        "print(\"DataFrame created from simulated images:\")\n",
        "display(df_simulated_ocr.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ce3da30"
      },
      "source": [
        "### Vectorize Extracted Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b2d2f1b"
      },
      "source": [
        "# === Step 8: Vectorización del texto extraído ===\n",
        "\n",
        "# Use the existing TfidfVectorizer or create a new one if needed\n",
        "# For this pipeline, we'll fit a new vectorizer on the OCR-extracted text\n",
        "# to create features specific to the text extracted from images.\n",
        "ocr_vectorizer = TfidfVectorizer()\n",
        "X_ocr = ocr_vectorizer.fit_transform(df_simulated_ocr[\"extracted_text\"]).toarray()\n",
        "\n",
        "# We also need to encode the intentions from the simulated data\n",
        "ocr_encoder = OneHotEncoder()\n",
        "y_ocr = ocr_encoder.fit_transform(df_simulated_ocr[[\"intent\"]]).toarray()\n",
        "\n",
        "print(\"Shape of vectorized OCR text features (X_ocr):\", X_ocr.shape)\n",
        "print(\"Shape of one-hot encoded OCR intentions (y_ocr):\", y_ocr.shape)\n",
        "\n",
        "# For training a model, you would typically split this OCR-based dataset\n",
        "# into training and testing sets, similar to the original text data pipeline.\n",
        "# X_ocr_train, X_ocr_test, y_ocr_train, y_ocr_test = train_test_split(X_ocr, y_ocr, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1fd3b76"
      },
      "source": [
        "### Train Classification Model on OCR Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40774c02"
      },
      "source": [
        "# === Step 9: Diseño y entrenamiento del modelo de clasificación ===\n",
        "\n",
        "# Split the OCR-based dataset into training and testing sets\n",
        "X_ocr_train, X_ocr_test, y_ocr_train, y_ocr_test = train_test_split(X_ocr, y_ocr, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the neural network model on the OCR data\n",
        "# Use the same NeuralNetwork class defined previously\n",
        "n_features_ocr = X_ocr_train.shape[1]\n",
        "n_classes_ocr = y_ocr_train.shape[1]\n",
        "nn_ocr = NeuralNetwork([n_features_ocr, 64, 32, n_classes_ocr], activation='relu')\n",
        "\n",
        "print(\"Starting training on OCR data...\")\n",
        "nn_ocr.fit(X_ocr_train, y_ocr_train, epochs=100, lr=0.01)\n",
        "print(\"Training on OCR data finished.\")\n",
        "\n",
        "# Note: For a real-world scenario with limited data like this,\n",
        "# you might consider data augmentation on images or using transfer learning\n",
        "# with a pre-trained model, but we'll stick to the simple NN for this example."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92a6efd5"
      },
      "source": [
        "### Evaluate Classification Model on OCR Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e9898c4"
      },
      "source": [
        "# === Step 10: Evaluación del modelo entrenado con datos de OCR ===\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_ocr_pred = nn_ocr.predict(X_ocr_test)\n",
        "\n",
        "# Get the true labels for the test set\n",
        "y_ocr_test_labels = np.argmax(y_ocr_test, axis=1)\n",
        "\n",
        "# Get the unique labels present in the OCR test set for the classification report\n",
        "unique_ocr_test_labels = np.unique(y_ocr_test_labels)\n",
        "\n",
        "print(\"Evaluation on OCR Data:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_ocr_test_labels, y_ocr_pred))\n",
        "\n",
        "# Get the target names from the OCR encoder\n",
        "ocr_target_names = ocr_encoder.categories_[0]\n",
        "\n",
        "# Print the classification report, specifying labels present in the test set\n",
        "print(classification_report(y_ocr_test_labels, y_ocr_pred, target_names=ocr_target_names, labels=unique_ocr_test_labels, zero_division=0))\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_ocr_test_labels, y_ocr_pred, labels=unique_ocr_test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2e56597"
      },
      "source": [
        "### Implement End-to-End Prediction Pipeline for Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d395fbf6"
      },
      "source": [
        "# === Step 11: Implementación de la predicción con imágenes ===\n",
        "\n",
        "def predict_intention_from_image(image_path, vectorizer, encoder, neural_network):\n",
        "    \"\"\"\n",
        "    Predicts the banking intention from a document image.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the document image file.\n",
        "        vectorizer (TfidfVectorizer): Trained TF-IDF vectorizer.\n",
        "        encoder (OneHotEncoder): Trained One-Hot Encoder for intentions.\n",
        "        neural_network (NeuralNetwork): Trained neural network model.\n",
        "\n",
        "    Returns:\n",
        "        str: Predicted banking intention.\n",
        "        str: Extracted and cleaned text from the image.\n",
        "    \"\"\"\n",
        "    # 1. Preprocess the image\n",
        "    preprocessed_img = preprocess_image_for_ocr(image_path)\n",
        "\n",
        "    if preprocessed_img is None:\n",
        "        return \"Prediction Error: Could not preprocess image.\", \"\"\n",
        "\n",
        "    # 2. Extract text with OCR\n",
        "    extracted_text = pytesseract.image_to_string(Image.fromarray(preprocessed_img), lang='spa')\n",
        "\n",
        "    # 3. Clean the extracted text\n",
        "    cleaned_text = clean_extracted_text(extracted_text)\n",
        "\n",
        "    if not cleaned_text:\n",
        "        return \"Prediction Error: No meaningful text extracted from image.\", cleaned_text\n",
        "\n",
        "    # 4. Vectorize the cleaned text\n",
        "    # Use the *fitted* vectorizer to transform the new text\n",
        "    X_new = vectorizer.transform([cleaned_text]).toarray() # transform expects an iterable\n",
        "\n",
        "    # 5. Predict intention\n",
        "    predictions = neural_network.predict(X_new)\n",
        "\n",
        "    # 6. Decode prediction\n",
        "    predicted_intent = encoder.categories_[0][predictions][0] # Get the single predicted label\n",
        "\n",
        "    return predicted_intent, cleaned_text\n",
        "\n",
        "# === Step 12: Pruebas con imágenes simuladas ===\n",
        "print(\"Testing the end-to-end pipeline on simulated images:\")\n",
        "\n",
        "# Get the list of simulated document images\n",
        "image_files_for_test = sorted([f for f in os.listdir(\"simulated_docs\") if f.endswith(\".png\")])\n",
        "\n",
        "if image_files_for_test:\n",
        "    # Use the trained OCR-based model and preprocessors\n",
        "    # Assuming nn_ocr, ocr_vectorizer, and ocr_encoder are available from previous steps\n",
        "    if 'nn_ocr' in locals() and 'ocr_vectorizer' in locals() and 'ocr_encoder' in locals():\n",
        "        for image_file in image_files_for_test:\n",
        "            image_path = os.path.join(\"simulated_docs\", image_file)\n",
        "            predicted_intent, extracted_cleaned_text = predict_intention_from_image(image_path, ocr_vectorizer, ocr_encoder, nn_ocr)\n",
        "            print(f\"Image: {image_file} -> Extracted Text: '{extracted_cleaned_text}' -> Predicted Intent: {predicted_intent}\")\n",
        "    else:\n",
        "        print(\"Error: OCR model (nn_ocr), vectorizer (ocr_vectorizer), or encoder (ocr_encoder) not found. Please run training steps first.\")\n",
        "else:\n",
        "    print(\"No simulated document images found in 'simulated_docs' directory for testing.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 1. INSTALACIÓN DE LIBRERÍAS\n",
        "# =========================\n",
        "!pip install pillow faker pytesseract opencv-python pandas\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import pytesseract\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from faker import Faker\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Configuración de Faker para español\n",
        "fake = Faker('es_ES')\n",
        "\n",
        "# Directorios\n",
        "os.makedirs(\"documentos_img\", exist_ok=True)\n",
        "os.makedirs(\"procesados\", exist_ok=True)\n",
        "\n",
        "# =========================\n",
        "# 2. GENERAR DOCUMENTOS SIMULADOS\n",
        "# =========================\n",
        "def generar_documento(tipo, idx):\n",
        "    ancho, alto = 800, 400\n",
        "    img = Image.new('RGB', (ancho, alto), color='white')\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    # Fuente\n",
        "    try:\n",
        "        font_title = ImageFont.truetype(\"arial.ttf\", 30)\n",
        "        font_text = ImageFont.truetype(\"arial.ttf\", 20)\n",
        "    except:\n",
        "        font_title = ImageFont.load_default()\n",
        "        font_text = ImageFont.load_default()\n",
        "\n",
        "    # Datos simulados\n",
        "    nombre = fake.name()\n",
        "    fecha = fake.date_between(start_date='-1y', end_date='today').strftime(\"%d/%m/%Y\")\n",
        "    valor = f\"${random.randint(100, 5000):,}.{random.randint(0,99):02}\".replace(\",\", \".\")\n",
        "    numero_doc = f\"{tipo[:2].upper()}-{random.randint(100000,999999)}\"\n",
        "    banco = \"Banco Digital\"\n",
        "\n",
        "    # Encabezado\n",
        "    draw.text((20, 20), banco, fill=\"black\", font=font_title)\n",
        "    draw.text((20, 60), f\"Tipo de documento: {tipo}\", fill=\"black\", font=font_text)\n",
        "    draw.text((20, 90), f\"Número: {numero_doc}\", fill=\"black\", font=font_text)\n",
        "\n",
        "    # Datos principales\n",
        "    draw.text((20, 140), f\"Nombre del cliente: {nombre}\", fill=\"black\", font=font_text)\n",
        "    draw.text((20, 180), f\"Fecha: {fecha}\", fill=\"black\", font=font_text)\n",
        "    draw.text((20, 220), f\"Valor: {valor} USD\", fill=\"black\", font=font_text)\n",
        "\n",
        "    # Firma simulada\n",
        "    draw.text((20, 300), \"Firma:\", fill=\"black\", font=font_text)\n",
        "    draw.line((100, 320, 300, 280), fill=\"black\", width=2)\n",
        "\n",
        "    # Guardar imagen\n",
        "    filename = f\"documentos_img/{tipo}_{idx}.png\"\n",
        "    img.save(filename)\n",
        "\n",
        "    return {\n",
        "        \"tipo_documento\": tipo,\n",
        "        \"nombre_cliente\": nombre,\n",
        "        \"fecha\": fecha,\n",
        "        \"valor\": valor,\n",
        "        \"numero_documento\": numero_doc,\n",
        "        \"banco\": banco,\n",
        "        \"ruta_imagen\": filename\n",
        "    }\n",
        "\n",
        "# Tipos de documentos\n",
        "tipos_documentos = [\"Cheque\", \"Estado de cuenta\", \"Recibo de depósito\", \"Comprobante de transferencia\"]\n",
        "\n",
        "# Generar dataset de imágenes\n",
        "datos_generados = []\n",
        "for tipo in tipos_documentos:\n",
        "    for i in range(20):  # 20 por tipo\n",
        "        datos_generados.append(generar_documento(tipo, i+1))\n",
        "\n",
        "df = pd.DataFrame(datos_generados)\n",
        "df.to_csv(\"procesados/dataset_original.csv\", index=False)\n",
        "\n",
        "print(\"✅ Imágenes generadas y datos guardados en 'procesados/dataset_original.csv'\")\n",
        "\n",
        "# =========================\n",
        "# 3. OCR PARA EXTRAER DATOS\n",
        "# =========================\n",
        "pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"  # Ruta en Google Colab\n",
        "\n",
        "def extraer_datos_ocr(ruta_img):\n",
        "    img = cv2.imread(ruta_img)\n",
        "\n",
        "    # Preprocesamiento\n",
        "    gris = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    _, umbral = cv2.threshold(gris, 150, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # OCR\n",
        "    texto = pytesseract.image_to_string(umbral, lang='spa')\n",
        "\n",
        "    return texto\n",
        "\n",
        "# Procesar imágenes y guardar texto\n",
        "resultados_ocr = []\n",
        "for doc in datos_generados:\n",
        "    texto_extraido = extraer_datos_ocr(doc[\"ruta_imagen\"])\n",
        "    resultados_ocr.append({\n",
        "        \"ruta_imagen\": doc[\"ruta_imagen\"],\n",
        "        \"texto_ocr\": texto_extraido\n",
        "    })\n",
        "\n",
        "df_ocr = pd.DataFrame(resultados_ocr)\n",
        "df_ocr.to_csv(\"procesados/resultados_ocr.csv\", index=False)\n",
        "\n",
        "print(\"✅ OCR completado y guardado en 'procesados/resultados_ocr.csv'\")"
      ],
      "metadata": {
        "id": "ZmezTniwUG_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vw7YCqowXsyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87fe0cc2"
      },
      "source": [
        "### Vectorize Extracted Text from 80 Simulated Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83a964f1"
      },
      "source": [
        "# === Step 8: Vectorización del texto extraído (de 80 imágenes) ===\n",
        "\n",
        "# Load the OCR results DataFrame\n",
        "import pandas as pd\n",
        "df_ocr = pd.read_csv(\"procesados/resultados_ocr.csv\")\n",
        "\n",
        "# Handle potential missing values in text column before vectorization\n",
        "df_ocr.dropna(subset=['texto_ocr'], inplace=True)\n",
        "df_ocr['texto_ocr'] = df_ocr['texto_ocr'].fillna('') # Replace NaN with empty string\n",
        "\n",
        "\n",
        "# Apply TF-IDF vectorization\n",
        "# For this larger dataset, we'll fit a new vectorizer\n",
        "large_ocr_vectorizer = TfidfVectorizer()\n",
        "X_large_ocr = large_ocr_vectorizer.fit_transform(df_ocr[\"texto_ocr\"]).toarray()\n",
        "\n",
        "print(\"Shape of vectorized OCR text features (X_large_ocr):\", X_large_ocr.shape)\n",
        "\n",
        "# We will need the corresponding labels from the original generated data for training\n",
        "# This will be addressed in the next step (preparing training data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49bc32f4"
      },
      "source": [
        "### Prepare Combined Dataset for Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90bacbf5"
      },
      "source": [
        "# === Step 7 (Revisited): Preparar conjunto de datos combinado ===\n",
        "\n",
        "# Load the OCR results DataFrame\n",
        "df_ocr = pd.read_csv(\"procesados/resultados_ocr.csv\")\n",
        "\n",
        "# Load the original dataset with labels\n",
        "df_original = pd.read_csv(\"procesados/dataset_original.csv\")\n",
        "\n",
        "# Merge the two dataframes based on the image path\n",
        "# Ensure the column names used for merging match in both dataframes\n",
        "df_combined = pd.merge(df_ocr, df_original[['ruta_imagen', 'tipo_documento']], on='ruta_imagen')\n",
        "\n",
        "# Rename 'tipo_documento' to 'intencion' for consistency with the original text data\n",
        "df_combined = df_combined.rename(columns={'tipo_documento': 'intencion'})\n",
        "\n",
        "# Clean the extracted text again, just in case (optional, but good practice)\n",
        "# Use the existing limpiar_texto function or create a new one if needed\n",
        "# For consistency, let's use the existing one, assuming it's suitable for OCR output\n",
        "def limpiar_texto_ocr(texto):\n",
        "    # Ensure text is a string\n",
        "    if not isinstance(texto, str):\n",
        "        return \"\"\n",
        "    texto = texto.lower()\n",
        "    # Keep a broader range of characters for OCR output initially\n",
        "    texto = re.sub(r\"[^a-záéíóúñ0-9\\s.,!?;:]\", \"\", texto) # Example: keep some punctuation\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "    return texto\n",
        "\n",
        "\n",
        "df_combined[\"texto_ocr_cleaned\"] = df_combined[\"texto_ocr\"].apply(limpiar_texto_ocr)\n",
        "\n",
        "# Verify the combined dataframe\n",
        "print(\"Combined DataFrame shape:\", df_combined.shape)\n",
        "display(df_combined.head())\n",
        "\n",
        "# Now, vectorize the cleaned OCR text from the combined dataframe\n",
        "# Use a new vectorizer fitted on this combined data, or the one from Step 8 if preferred\n",
        "# Using the vectorizer from Step 8 is more consistent if we want to use its vocabulary\n",
        "if 'large_ocr_vectorizer' in locals():\n",
        "    X_combined_ocr = large_ocr_vectorizer.transform(df_combined[\"texto_ocr_cleaned\"]).toarray()\n",
        "    print(\"Shape of vectorized combined OCR text features (X_combined_ocr) using existing vectorizer:\", X_combined_ocr.shape)\n",
        "else:\n",
        "    # If the vectorizer from Step 8 is not available, fit a new one\n",
        "    combined_ocr_vectorizer = TfidfVectorizer()\n",
        "    X_combined_ocr = combined_ocr_vectorizer.fit_transform(df_combined[\"texto_ocr_cleaned\"]).toarray()\n",
        "    large_ocr_vectorizer = combined_ocr_vectorizer # Update the variable for later use\n",
        "    print(\"Shape of vectorized combined OCR text features (X_combined_ocr) using a new vectorizer:\", X_combined_ocr.shape)\n",
        "\n",
        "\n",
        "# Encode the intention labels from the combined dataframe\n",
        "combined_ocr_encoder = OneHotEncoder()\n",
        "y_combined_ocr = combined_ocr_encoder.fit_transform(df_combined[[\"intencion\"]]).toarray()\n",
        "\n",
        "print(\"Shape of one-hot encoded combined intentions (y_combined_ocr):\", y_combined_ocr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "321336f2"
      },
      "source": [
        "### Train Classification Model on Combined OCR Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c16a21b6"
      },
      "source": [
        "# === Step 9: Diseño y entrenamiento del modelo de clasificación (con datos combinados) ===\n",
        "\n",
        "# Split the combined OCR-based dataset into training and testing sets\n",
        "X_combined_ocr_train, X_combined_ocr_test, y_combined_ocr_train, y_combined_ocr_test = train_test_split(X_combined_ocr, y_combined_ocr, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the neural network model on the combined OCR data\n",
        "n_features_combined_ocr = X_combined_ocr_train.shape[1]\n",
        "n_classes_combined_ocr = y_combined_ocr_train.shape[1]\n",
        "nn_combined_ocr = NeuralNetwork([n_features_combined_ocr, 64, 32, n_classes_combined_ocr], activation='relu')\n",
        "\n",
        "print(\"Starting training on combined OCR data...\")\n",
        "nn_combined_ocr.fit(X_combined_ocr_train, y_combined_ocr_train, epochs=100, lr=0.01)\n",
        "print(\"Training on combined OCR data finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9192ec36"
      },
      "source": [
        "### Evaluate Classification Model on Combined OCR Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cc6e172"
      },
      "source": [
        "# === Step 10: Evaluación del modelo entrenado con datos combinados de OCR ===\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_combined_ocr_pred = nn_combined_ocr.predict(X_combined_ocr_test)\n",
        "\n",
        "# Get the true labels for the test set\n",
        "y_combined_ocr_test_labels = np.argmax(y_combined_ocr_test, axis=1)\n",
        "\n",
        "# Get the unique labels present in the combined OCR test set for the classification report\n",
        "unique_combined_ocr_test_labels = np.unique(y_combined_ocr_test_labels)\n",
        "\n",
        "print(\"Evaluation on Combined OCR Data:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_combined_ocr_test_labels, y_combined_ocr_pred))\n",
        "\n",
        "# Get the target names from the combined OCR encoder\n",
        "combined_ocr_target_names = combined_ocr_encoder.categories_[0]\n",
        "\n",
        "# Print the classification report, specifying labels present in the test set\n",
        "print(classification_report(y_combined_ocr_test_labels, y_combined_ocr_pred, target_names=combined_ocr_target_names, labels=unique_combined_ocr_test_labels, zero_division=0))\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_combined_ocr_test_labels, y_combined_ocr_pred, labels=unique_combined_ocr_test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03386813"
      },
      "source": [
        "### Implement End-to-End Prediction Pipeline for Images (using combined data model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b3e6eb3"
      },
      "source": [
        "# === Step 11: Implementación de la predicción con imágenes (usando el modelo entrenado con datos combinados) ===\n",
        "\n",
        "def predict_intention_from_image_combined(image_path, vectorizer, encoder, neural_network):\n",
        "    \"\"\"\n",
        "    Predicts the banking intention from a document image using the model trained on combined OCR data.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the document image file.\n",
        "        vectorizer (TfidfVectorizer): Trained TF-IDF vectorizer (fitted on combined OCR data).\n",
        "        encoder (OneHotEncoder): Trained One-Hot Encoder for intentions (fitted on combined OCR intentions).\n",
        "        neural_network (NeuralNetwork): Trained neural network model (trained on combined OCR data).\n",
        "\n",
        "    Returns:\n",
        "        str: Predicted banking intention.\n",
        "        str: Extracted and cleaned text from the image.\n",
        "    \"\"\"\n",
        "    # 1. Preprocess the image\n",
        "    preprocessed_img = preprocess_image_for_ocr(image_path)\n",
        "\n",
        "    if preprocessed_img is None:\n",
        "        return \"Prediction Error: Could not preprocess image.\", \"\"\n",
        "\n",
        "    # 2. Extract text with OCR\n",
        "    extracted_text = pytesseract.image_to_string(Image.fromarray(preprocessed_img), lang='spa')\n",
        "\n",
        "    # 3. Clean the extracted text\n",
        "    # Use the specific cleaning function for OCR text if different, otherwise use the generic one\n",
        "    cleaned_text = limpiar_texto_ocr(extracted_text) # Using the cleaning function defined for OCR text\n",
        "\n",
        "    if not cleaned_text:\n",
        "        return \"Prediction Error: No meaningful text extracted from image.\", cleaned_text\n",
        "\n",
        "    # 4. Vectorize the cleaned text\n",
        "    # Use the *fitted* vectorizer to transform the new text\n",
        "    X_new = vectorizer.transform([cleaned_text]).toarray() # transform expects an iterable\n",
        "\n",
        "    # 5. Predict intention\n",
        "    predictions = neural_network.predict(X_new)\n",
        "\n",
        "    # 6. Decode prediction\n",
        "    predicted_intent = encoder.categories_[0][predictions][0] # Get the single predicted label\n",
        "\n",
        "    return predicted_intent, cleaned_text\n",
        "\n",
        "# === Step 12: Pruebas con imágenes simuladas (usando el modelo entrenado con datos combinados) ===\n",
        "print(\"Testing the end-to-end pipeline on simulated images (using combined data model):\")\n",
        "\n",
        "# Get the list of simulated document images\n",
        "image_files_for_test = sorted([f for f in os.listdir(\"documentos_img\") if f.endswith(\".png\")]) # Use the directory with 80 images\n",
        "\n",
        "if image_files_for_test:\n",
        "    # Use the trained combined OCR model and preprocessors\n",
        "    # Assuming nn_combined_ocr, large_ocr_vectorizer (or combined_ocr_vectorizer), and combined_ocr_encoder are available\n",
        "    if ('nn_combined_ocr' in locals() or 'nn_combined_ocr' in globals()) and \\\n",
        "       (('large_ocr_vectorizer' in locals() or 'large_ocr_vectorizer' in globals()) or \\\n",
        "        ('combined_ocr_vectorizer' in locals() or 'combined_ocr_vectorizer' in globals())) and \\\n",
        "       ('combined_ocr_encoder' in locals() or 'combined_ocr_encoder' in globals()):\n",
        "\n",
        "        # Determine which vectorizer variable is available\n",
        "        current_vectorizer = large_ocr_vectorizer if 'large_ocr_vectorizer' in locals() or 'large_ocr_vectorizer' in globals() else combined_ocr_vectorizer\n",
        "\n",
        "        for image_file in image_files_for_test:\n",
        "            image_path = os.path.join(\"documentos_img\", image_file)\n",
        "            predicted_intent, extracted_cleaned_text = predict_intention_from_image_combined(image_path, current_vectorizer, combined_ocr_encoder, nn_combined_ocr)\n",
        "            print(f\"Image: {image_file} -> Extracted Text: '{extracted_cleaned_text}' -> Predicted Intent: {predicted_intent}\")\n",
        "    else:\n",
        "        print(\"Error: Combined OCR model (nn_combined_ocr), vectorizer, or encoder not found. Please run training steps first.\")\n",
        "else:\n",
        "    print(\"No simulated document images found in 'documentos_img' directory for testing.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6bf841c"
      },
      "source": [
        "# EXPERIMENTACION\n",
        "Realiza un proceso de experimentación simple y fácil de comprender para comparar diferentes arquitecturas de redes neuronales, funciones de activación e hiperparámetros para la clasificación de intenciones bancarias basadas en texto y OCR, utilizando como baseline una regresión logística. Guarda los resultados de rendimiento y las curvas de entrenamiento.\n",
        "Asegurarse de tener listos los datos vectorizados y codificados del OCR (X_combined_ocr, y_combined_ocr) y dividirlos en conjuntos de entrenamiento y prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "064ced8e"
      },
      "source": [
        "# 1. Verify variables and their shapes\n",
        "print(\"Shape of X_combined_ocr:\", X_combined_ocr.shape if 'X_combined_ocr' in locals() or 'X_combined_ocr' in globals() else \"X_combined_ocr not found\")\n",
        "print(\"Shape of y_combined_ocr:\", y_combined_ocr.shape if 'y_combined_ocr' in locals() or 'y_combined_ocr' in globals() else \"y_combined_ocr not found\")\n",
        "\n",
        "# Check if variables exist and are not empty before splitting\n",
        "if ('X_combined_ocr' in locals() or 'X_combined_ocr' in globals()) and \\\n",
        "   ('y_combined_ocr' in locals() or 'y_combined_ocr' in globals()) and \\\n",
        "   X_combined_ocr.shape[0] > 0 and y_combined_ocr.shape[0] > 0:\n",
        "\n",
        "    # 3. Split the data into training and testing sets\n",
        "    X_combined_ocr_train, X_combined_ocr_test, y_combined_ocr_train, y_combined_ocr_test = train_test_split(\n",
        "        X_combined_ocr, y_combined_ocr, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # 4. Print the shapes of the resulting sets\n",
        "    print(\"\\nShapes after splitting:\")\n",
        "    print(\"X_combined_ocr_train shape:\", X_combined_ocr_train.shape)\n",
        "    print(\"X_combined_ocr_test shape:\", X_combined_ocr_test.shape)\n",
        "    print(\"y_combined_ocr_train shape:\", y_combined_ocr_train.shape)\n",
        "    print(\"y_combined_ocr_test shape:\", y_combined_ocr_test.shape)\n",
        "\n",
        "else:\n",
        "    print(\"\\nX_combined_ocr or y_combined_ocr not found or are empty. Cannot split data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e09e15cf"
      },
      "source": [
        "## Definir configuraciones de red neuronal a probar\n",
        "\n",
        "### Subtask:\n",
        "Especificar claramente las 3 arquitecturas (número de capas y neuronas) y las funciones de activación (ReLU, Sigmoid, Tanh) que se experimentarán. También definir los hiperparámetros (learning rate, epochs) para cada prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77d2ce04"
      },
      "source": [
        "# Get input and output dimensions from the prepared data\n",
        "input_dim = X_combined_ocr_train.shape[1]\n",
        "output_dim = y_combined_ocr_train.shape[1]\n",
        "\n",
        "# Define the neural network configurations for experimentation\n",
        "nn_experiments = [\n",
        "    {\n",
        "        'name': 'Simple_NN_ReLU',\n",
        "        'architecture': [input_dim, 32, output_dim], # Simple architecture: input -> 32 neurons -> output\n",
        "        'activation': 'relu',\n",
        "        'lr': 0.01,\n",
        "        'epochs': 100\n",
        "    },\n",
        "    {\n",
        "        'name': 'Medium_NN_ReLU',\n",
        "        'architecture': [input_dim, 64, 32, output_dim], # Medium architecture: input -> 64 -> 32 -> output\n",
        "        'activation': 'relu',\n",
        "        'lr': 0.01,\n",
        "        'epochs': 100\n",
        "    },\n",
        "    {\n",
        "        'name': 'Medium_NN_Sigmoid',\n",
        "        'architecture': [input_dim, 64, 32, output_dim], # Same medium architecture, but with Sigmoid activation\n",
        "        'activation': 'sigmoid', # Note: Sigmoid should be used for output, but this config tests it for hidden layers\n",
        "        'lr': 0.01,\n",
        "        'epochs': 100\n",
        "    },\n",
        "     {\n",
        "        'name': 'Medium_NN_Tanh',\n",
        "        'architecture': [input_dim, 64, 32, output_dim], # Same medium architecture, but with Tanh activation\n",
        "        'activation': 'tanh',\n",
        "        'lr': 0.01,\n",
        "        'epochs': 100\n",
        "    },\n",
        "    {\n",
        "        'name': 'Medium_NN_ReLU_HigherLR',\n",
        "        'architecture': [input_dim, 64, 32, output_dim], # Medium architecture with a higher learning rate\n",
        "        'activation': 'relu',\n",
        "        'lr': 0.05, # Higher learning rate\n",
        "        'epochs': 100\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Defined Neural Network Experiment Configurations:\")\n",
        "for exp in nn_experiments:\n",
        "    print(f\"- Name: {exp['name']}, Architecture: {exp['architecture']}, Activation: {exp['activation']}, LR: {exp['lr']}, Epochs: {exp['epochs']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b0415e3"
      },
      "source": [
        "## Implementar y entrenar el modelo baseline (regresión logística)\n",
        "\n",
        "### Subtask:\n",
        "Configurar y entrenar un modelo de Regresión Logística de scikit-learn con los mismos datos de entrenamiento (`X_combined_ocr_train`, `y_combined_ocr_train`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c610c5a1"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Convert one-hot encoded training labels back to single-column labels for scikit-learn\n",
        "y_combined_ocr_train_labels = np.argmax(y_combined_ocr_train, axis=1)\n",
        "\n",
        "# 2. Instantiate a LogisticRegression model\n",
        "# Use 'auto' for multi_class and 'liblinear' as a suitable solver for small datasets\n",
        "# Increase max_iter if convergence issues arise\n",
        "log_reg_model = LogisticRegression(multi_class='auto', solver='liblinear', random_state=42, max_iter=1000)\n",
        "\n",
        "# 3. Train the LogisticRegression model\n",
        "print(\"Starting training of Logistic Regression model...\")\n",
        "log_reg_model.fit(X_combined_ocr_train, y_combined_ocr_train_labels)\n",
        "print(\"Training of Logistic Regression model finished.\")\n",
        "\n",
        "# 4. The trained model is stored in the 'log_reg_model' variable."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a24f340d"
      },
      "source": [
        "## Evaluar el baseline\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained Logistic Regression model on the test set (`X_combined_ocr_test`, `y_combined_ocr_test`) and record its performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0de5451d"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Convert one-hot encoded test labels back to single-column labels\n",
        "y_combined_ocr_test_labels = np.argmax(y_combined_ocr_test, axis=1)\n",
        "\n",
        "# 5. Make predictions on the test set\n",
        "y_combined_ocr_log_reg_pred = log_reg_model.predict(X_combined_ocr_test)\n",
        "\n",
        "# 6. Evaluate the model\n",
        "accuracy = accuracy_score(y_combined_ocr_test_labels, y_combined_ocr_log_reg_pred)\n",
        "report = classification_report(y_combined_ocr_test_labels, y_combined_ocr_log_reg_pred, target_names=combined_ocr_encoder.categories_[0], zero_division=0)\n",
        "conf_matrix = confusion_matrix(y_combined_ocr_test_labels, y_combined_ocr_log_reg_pred, labels=np.unique(y_combined_ocr_test_labels))\n",
        "\n",
        "print(\"Evaluation of Logistic Regression Model on Combined OCR Data:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Store the results\n",
        "log_reg_results = {\n",
        "    'model_name': 'Logistic Regression',\n",
        "    'accuracy': accuracy,\n",
        "    'classification_report': report,\n",
        "    'confusion_matrix': conf_matrix.tolist() # Convert numpy array to list for easier storage/viewing\n",
        "}\n",
        "\n",
        "print(\"\\nLogistic Regression results stored.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "166a4711"
      },
      "source": [
        "## Entrenar y evaluar cada configuración de red neuronal\n",
        "\n",
        "### Subtask:\n",
        "Iterar sobre las configuraciones de red neuronal definidas en `nn_experiments`. Para cada configuración, inicializar la red, entrenarla y evaluarla, registrando la pérdida por epoch y las métricas de rendimiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9270fa25"
      },
      "source": [
        "# Initialize list to store experiment results\n",
        "nn_experiment_results = []\n",
        "\n",
        "# Loop through each configuration\n",
        "for exp in nn_experiments:\n",
        "    print(f\"\\n--- Training Experiment: {exp['name']} ---\")\n",
        "    print(f\"  Architecture: {exp['architecture']}, Activation: {exp['activation']}, LR: {exp['lr']}, Epochs: {exp['epochs']}\")\n",
        "\n",
        "    # Instantiate the neural network\n",
        "    nn = NeuralNetwork(layers=exp['architecture'], activation=exp['activation'], seed=42)\n",
        "\n",
        "    # Train the neural network and get loss history\n",
        "    # Assuming X_combined_ocr_train and y_combined_ocr_train are available\n",
        "    loss_history = nn.fit(X_combined_ocr_train, y_combined_ocr_train, epochs=exp['epochs'], lr=exp['lr'])\n",
        "\n",
        "    print(f\"  Training complete for {exp['name']}.\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    # Assuming X_combined_ocr_test and y_combined_ocr_test are available\n",
        "    y_combined_ocr_test_labels = np.argmax(y_combined_ocr_test, axis=1)\n",
        "\n",
        "    # Make sure y_combined_ocr_pred is computed correctly before passing to metrics\n",
        "    y_combined_ocr_pred = nn.predict(X_combined_ocr_test)\n",
        "\n",
        "    # Get the unique labels present in the test set for the classification report\n",
        "    unique_combined_ocr_test_labels = np.unique(y_combined_ocr_test_labels)\n",
        "    # Get the target names from the encoder (assuming combined_ocr_encoder is available)\n",
        "    combined_ocr_target_names = combined_ocr_encoder.categories_[0]\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(y_combined_ocr_test_labels, y_combined_ocr_pred)\n",
        "    report = classification_report(y_combined_ocr_test_labels, y_combined_ocr_pred, target_names=combined_ocr_target_names, labels=unique_combined_ocr_test_labels, zero_division=0)\n",
        "    conf_matrix = confusion_matrix(y_combined_ocr_test_labels, y_combined_ocr_pred, labels=unique_combined_ocr_test_labels)\n",
        "\n",
        "    print(f\"\\n  Evaluation for {exp['name']}:\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(\"  Classification Report:\\n\", report)\n",
        "    print(\"  Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "    # Store the results\n",
        "    nn_experiment_results.append({\n",
        "        'name': exp['name'],\n",
        "        'architecture': exp['architecture'],\n",
        "        'activation': exp['activation'],\n",
        "        'lr': exp['lr'],\n",
        "        'epochs': exp['epochs'],\n",
        "        'accuracy': accuracy,\n",
        "        'classification_report': report,\n",
        "        'confusion_matrix': conf_matrix.tolist(), # Convert numpy array to list\n",
        "        'loss_history': loss_history # Store the recorded loss history\n",
        "    })\n",
        "\n",
        "print(\"\\n--- All Neural Network Experiments Complete ---\")\n",
        "print(f\"Results stored for {len(nn_experiment_results)} experiments.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a838fe2"
      },
      "source": [
        "## Recopilar y guardar todos los resultados\n",
        "\n",
        "### Subtask:\n",
        "Consolidar las métricas de rendimiento de la Regresión Logística y de todas las configuraciones de Red Neuronal probadas. Guardar estos resultados y las historias de pérdida (por ejemplo, en DataFrames o archivos pickle)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0339336"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Create a dictionary to hold all experiment results\n",
        "all_experiment_results = {}\n",
        "\n",
        "# 2. Add the log_reg_results dictionary\n",
        "# Assuming 'log_reg_results' is available from a previous step\n",
        "if 'log_reg_results' in locals() or 'log_reg_results' in globals():\n",
        "    all_experiment_results['Logistic Regression'] = log_reg_results\n",
        "    print(\"Added Logistic Regression results to consolidated results.\")\n",
        "else:\n",
        "    print(\"Warning: 'log_reg_results' not found. Skipping adding Logistic Regression results.\")\n",
        "\n",
        "\n",
        "# 3. Add the nn_experiment_results list\n",
        "# Assuming 'nn_experiment_results' is available from a previous step\n",
        "if 'nn_experiment_results' in locals() or 'nn_experiment_results' in globals():\n",
        "    all_experiment_results['Neural Networks'] = nn_experiment_results\n",
        "    print(f\"Added {len(nn_experiment_results)} Neural Network experiment results to consolidated results.\")\n",
        "else:\n",
        "    print(\"Warning: 'nn_experiment_results' not found. Skipping adding Neural Network results.\")\n",
        "\n",
        "\n",
        "# 4. Optionally, convert the nn_experiment_results list into a pandas DataFrame\n",
        "if 'nn_experiment_results' in locals() or 'nn_experiment_results' in globals():\n",
        "    # Create a list of dictionaries without the loss history for the DataFrame\n",
        "    nn_metrics_list = []\n",
        "    for result in nn_experiment_results:\n",
        "        metrics_dict = result.copy()\n",
        "        # Remove loss_history as it's not suitable for a simple metrics DataFrame\n",
        "        if 'loss_history' in metrics_dict:\n",
        "            del metrics_dict['loss_history']\n",
        "        nn_metrics_list.append(metrics_dict)\n",
        "\n",
        "    if nn_metrics_list:\n",
        "        df_nn_metrics = pd.DataFrame(nn_metrics_list)\n",
        "        print(\"\\nCreated DataFrame for Neural Network experiment metrics.\")\n",
        "        display(df_nn_metrics)\n",
        "    else:\n",
        "        df_nn_metrics = pd.DataFrame() # Create empty DataFrame if list is empty\n",
        "        print(\"\\nNeural Network metrics list is empty. No DataFrame created.\")\n",
        "else:\n",
        "    df_nn_metrics = pd.DataFrame() # Ensure df_nn_metrics is defined even if no NN results\n",
        "\n",
        "\n",
        "# 5. Save the complete results dictionary to a file using pickle\n",
        "try:\n",
        "    with open(\"experiment_results.pkl\", \"wb\") as f:\n",
        "        pickle.dump(all_experiment_results, f)\n",
        "    print(\"\\nComplete experiment results saved to 'experiment_results.pkl'.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError saving 'experiment_results.pkl': {e}\")\n",
        "\n",
        "\n",
        "# 6. Save the DataFrame of NN results (if created and not empty) to a CSV file\n",
        "if not df_nn_metrics.empty:\n",
        "    try:\n",
        "        df_nn_metrics.to_csv(\"nn_experiment_metrics.csv\", index=False)\n",
        "        print(\"Neural Network experiment metrics saved to 'nn_experiment_metrics.csv'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving 'nn_experiment_metrics.csv': {e}\")\n",
        "else:\n",
        "    print(\"\\nNo Neural Network metrics DataFrame to save to CSV.\")\n",
        "\n",
        "\n",
        "# 7. Print a confirmation message (already included in steps 5 and 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c85d670"
      },
      "source": [
        "## Comparar y visualizar resultados\n",
        "\n",
        "### Subtask:\n",
        "Presentar una comparación clara del rendimiento de todos los modelos (Regresión Logística y las diferentes RN) utilizando tablas o gráficos sencillos (ej. gráfico de barras de precisión o F1-score). Visualizar las curvas de pérdida de las redes neuronales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10cba198"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the experiment results\n",
        "try:\n",
        "    with open(\"experiment_results.pkl\", \"rb\") as f:\n",
        "        all_experiment_results = pickle.load(f)\n",
        "    print(\"Experiment results loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: experiment_results.pkl not found. Please ensure the previous step to save results was completed.\")\n",
        "    all_experiment_results = None # Set to None to avoid errors in subsequent steps\n",
        "\n",
        "# Check if results were loaded\n",
        "if all_experiment_results:\n",
        "    # 2. Extract Logistic Regression and Neural Network results\n",
        "    log_reg_result = all_experiment_results.get('Logistic Regression')\n",
        "    nn_results_list = all_experiment_results.get('Neural Networks', [])\n",
        "\n",
        "    # 3. Create a pandas DataFrame from the Neural Network results\n",
        "    # Exclude loss_history for the comparison table\n",
        "    nn_comparison_list = []\n",
        "    for result in nn_results_list:\n",
        "        nn_comparison_list.append({\n",
        "            'name': result['name'],\n",
        "            'accuracy': result['accuracy']\n",
        "        })\n",
        "\n",
        "    df_comparison = pd.DataFrame(nn_comparison_list)\n",
        "\n",
        "    # 4. Add Logistic Regression results to the DataFrame\n",
        "    if log_reg_result:\n",
        "        log_reg_comparison = {'name': log_reg_result['model_name'], 'accuracy': log_reg_result['accuracy']}\n",
        "        df_comparison = pd.concat([df_comparison, pd.DataFrame([log_reg_comparison])], ignore_index=True)\n",
        "        print(\"\\nComparison data prepared, including Logistic Regression.\")\n",
        "    else:\n",
        "        print(\"\\nWarning: Logistic Regression results not found. Comparison table will only contain NN results.\")\n",
        "\n",
        "    # 5. Sort the DataFrame by accuracy\n",
        "    df_comparison_sorted = df_comparison.sort_values(by='accuracy', ascending=False)\n",
        "\n",
        "    print(\"\\nPerformance Comparison Table:\")\n",
        "    display(df_comparison_sorted)\n",
        "\n",
        "    # 6. Create a bar plot comparing accuracy\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x='name', y='accuracy', data=df_comparison_sorted, palette='viridis')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Model')\n",
        "    plt.title('Model Accuracy Comparison')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 7. and 8. Plot loss history for Neural Networks\n",
        "    if nn_results_list:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        for result in nn_results_list:\n",
        "            if 'loss_history' in result and result['loss_history']:\n",
        "                plt.plot(result['loss_history'], label=result['name'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Neural Network Training Loss History')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"\\nNo Neural Network loss history available to plot.\")\n",
        "\n",
        "else:\n",
        "    print(\"Cannot proceed with comparison and plotting as experiment results were not loaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "680c2a3c"
      },
      "source": [
        "# Task\n",
        "Implementar una pipeline multimodal simple en Python que pueda procesar tanto texto directo como imágenes de documentos bancarios simulados (\"dataset_bancario_espanol.csv\"), utilizando los modelos de clasificación de intenciones y OCR previamente desarrollados para predecir la intención del usuario a partir de la entrada, y proporcionar una respuesta básica basada en la intención predicha."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07eff593"
      },
      "source": [
        "## Definir los tipos de entrada\n",
        "\n",
        "### Subtask:\n",
        "Establecer claramente que el sistema puede recibir dos tipos principales de entrada: texto directo o una imagen de documento.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3933db9"
      },
      "source": [
        "# This is a placeholder cell to ensure the next command is a Markdown cell.\n",
        "# It will be deleted.\n",
        "pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34578eac"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new Markdown cell and explain the two input types for the multimodal pipeline as requested in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e05cccf"
      },
      "source": [
        "# %% [markdown]\n",
        "# ## Pipeline Multimodal: Tipos de Entrada\n",
        "#\n",
        "# El sistema de pipeline multimodal está diseñado para procesar la entrada del usuario de dos maneras principales:\n",
        "#\n",
        "# 1.  **Entrada de Texto Directo:** El usuario puede escribir directamente su consulta o solicitud como texto.\n",
        "#\n",
        "# 2.  **Entrada de Imagen de Documento:** El usuario puede proporcionar una imagen de un documento bancario simulado.\n",
        "#\n",
        "# En la implementación, la pipeline diferenciará entre estos dos tipos de entrada típicamente revisando el formato o tipo de la entrada proporcionada. Por ejemplo, una función de procesamiento podría aceptar una cadena (para texto directo) o una ruta de archivo/objeto de imagen (para una imagen), utilizando lógica condicional para aplicar el flujo de procesamiento adecuado (procesamiento de texto o procesamiento con OCR)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa04832c"
      },
      "source": [
        "## Manejar la entrada de texto\n",
        "\n",
        "### Subtask:\n",
        "Si la entrada es texto directo, utilizar el modelo de clasificación de intenciones bancarias basado en texto que ya entrenamos para predecir la intención.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e07399a4"
      },
      "source": [
        "import re # Import re if not already imported in the notebook\n",
        "import numpy as np # Import numpy if not already imported\n",
        "\n",
        "def predict_intention_from_text(text):\n",
        "    \"\"\"\n",
        "    Predicts the banking intention from direct text input.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text from the user.\n",
        "\n",
        "    Returns:\n",
        "        str: The predicted banking intention, or an error message.\n",
        "    \"\"\"\n",
        "    # Ensure the necessary models and preprocessors are available\n",
        "    if 'vectorizer' not in globals() and 'vectorizer' not in locals():\n",
        "        return \"Prediction Error: Text vectorizer model not found. Please ensure it is loaded or trained.\"\n",
        "    if 'nn' not in globals() and 'nn' not in locals():\n",
        "         return \"Prediction Error: Text classification model not found. Please ensure it is loaded or trained.\"\n",
        "    if 'encoder' not in globals() and 'encoder' not in locals():\n",
        "        return \"Prediction Error: Intention encoder not found. Please ensure it is loaded or trained.\"\n",
        "    if 'limpiar_texto' not in globals() and 'limpiar_texto' not in locals():\n",
        "         return \"Prediction Error: Text cleaning function 'limpiar_texto' not found.\"\n",
        "\n",
        "\n",
        "    # 1. Clean the input text\n",
        "    cleaned_text = limpiar_texto(text)\n",
        "\n",
        "    # 2. Check if the cleaned text is empty\n",
        "    if not cleaned_text:\n",
        "        return \"No meaningful text found after cleaning.\"\n",
        "\n",
        "    # 3. Vectorize the cleaned text\n",
        "    try:\n",
        "        X_new = vectorizer.transform([cleaned_text]).toarray()\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error during vectorization: {e}\"\n",
        "\n",
        "\n",
        "    # 4. Predict intention\n",
        "    try:\n",
        "        predictions = nn.predict(X_new)\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error during prediction: {e}\"\n",
        "\n",
        "    # 5. Decode prediction\n",
        "    try:\n",
        "        # Ensure the encoder categories are accessible and the prediction index is valid\n",
        "        if hasattr(encoder, 'categories_') and encoder.categories_ and predictions[0] < len(encoder.categories_[0]):\n",
        "             predicted_intent = encoder.categories_[0][predictions][0]\n",
        "        else:\n",
        "             return f\"Prediction Error: Could not decode prediction {predictions[0]}. Invalid index or encoder issue.\"\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error during decoding: {e}\"\n",
        "\n",
        "    # 6. Return the predicted intention\n",
        "    return predicted_intent\n",
        "\n",
        "# Example Usage (Optional: for testing the function)\n",
        "# Assuming vectorizer, nn, encoder, and limpiar_texto are loaded/defined from previous steps\n",
        "# test_text = \"Quiero consultar mi saldo\"\n",
        "# predicted = predict_intention_from_text(test_text)\n",
        "# print(f\"Input Text: '{test_text}' -> Predicted Intent: {predicted}\")\n",
        "\n",
        "# test_text_empty = \"\"\n",
        "# predicted_empty = predict_intention_from_text(test_text_empty)\n",
        "# print(f\"Input Text: '{test_text_empty}' -> Predicted Intent: {predicted_empty}\")\n",
        "\n",
        "# test_text_special_chars = \"!!! Necesito_información_sobre... un Préstamo?!!!\"\n",
        "# predicted_special = predict_intention_from_text(test_text_special_chars)\n",
        "# print(f\"Input Text: '{test_text_special_chars}' -> Predicted Intent: {predicted_special}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6ba6d88"
      },
      "source": [
        "## Manejar la entrada de imagen\n",
        "\n",
        "### Subtask:\n",
        "Implement the logic to handle image input for intention classification. This involves preprocessing the image, performing OCR to extract text, cleaning the extracted text, and then using the appropriate classification model to predict the intention based on the text from the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5ca0237"
      },
      "source": [
        "def predict_intention_from_image(image_path, vectorizer, encoder, neural_network):\n",
        "    \"\"\"\n",
        "    Predicts the banking intention from a document image.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the document image file.\n",
        "        vectorizer (TfidfVectorizer): Trained TF-IDF vectorizer (fitted on combined OCR data).\n",
        "        encoder (OneHotEncoder): Trained One-Hot Encoder for intentions (fitted on combined OCR intentions).\n",
        "        neural_network (NeuralNetwork): Trained neural network model (trained on combined OCR data).\n",
        "\n",
        "    Returns:\n",
        "        str: Predicted banking intention.\n",
        "        str: Extracted and cleaned text from the image.\n",
        "    \"\"\"\n",
        "    # 1. Preprocess the image\n",
        "    preprocessed_img = preprocess_image_for_ocr(image_path)\n",
        "\n",
        "    if preprocessed_img is None:\n",
        "        return \"Prediction Error: Could not preprocess image.\", \"\"\n",
        "\n",
        "    # 2. Extract text with OCR\n",
        "    try:\n",
        "        extracted_text = pytesseract.image_to_string(Image.fromarray(preprocessed_img), lang='spa')\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error during OCR extraction: {e}\", \"\"\n",
        "\n",
        "\n",
        "    # 3. Clean the extracted text\n",
        "    # Use the specific cleaning function for OCR text if different, otherwise use the generic one\n",
        "    # Assuming limpiar_texto_ocr is available from previous steps\n",
        "    if 'limpiar_texto_ocr' in globals() or 'limpiar_texto_ocr' in locals():\n",
        "        cleaned_text = limpiar_texto_ocr(extracted_text)\n",
        "    elif 'limpiar_texto' in globals() or 'limpiar_texto' in locals():\n",
        "         cleaned_text = limpiar_texto(extracted_text)\n",
        "    else:\n",
        "         return \"Prediction Error: Text cleaning function not found.\", extracted_text\n",
        "\n",
        "\n",
        "    if not cleaned_text:\n",
        "        return \"Prediction Error: No meaningful text extracted or cleaned from image.\", cleaned_text\n",
        "\n",
        "    # 4. Vectorize the cleaned text\n",
        "    try:\n",
        "        # Use the *fitted* vectorizer to transform the new text\n",
        "        X_new = vectorizer.transform([cleaned_text]).toarray() # transform expects an iterable\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error during vectorization: {e}\", cleaned_text\n",
        "\n",
        "    # 5. Predict intention\n",
        "    try:\n",
        "        predictions = neural_network.predict(X_new)\n",
        "    except Exception as e:\n",
        "         return f\"Prediction Error during neural network prediction: {e}\", cleaned_text\n",
        "\n",
        "\n",
        "    # 6. Decode prediction\n",
        "    try:\n",
        "        # Ensure the encoder categories are accessible and the prediction index is valid\n",
        "        if hasattr(encoder, 'categories_') and encoder.categories_ and predictions[0] < len(encoder.categories_[0]):\n",
        "             predicted_intent = encoder.categories_[0][predictions][0]\n",
        "        else:\n",
        "             return f\"Prediction Error: Could not decode prediction {predictions[0]}. Invalid index or encoder issue.\", cleaned_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error during decoding prediction: {e}\", cleaned_text\n",
        "\n",
        "    # 7. Return the predicted intention and the extracted/cleaned text\n",
        "    return predicted_intent, cleaned_text\n",
        "\n",
        "print(\"Defined predict_intention_from_image function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fa18023"
      },
      "source": [
        "# === Test the predict_intention_from_image function ===\n",
        "print(\"Testing the predict_intention_from_image function with simulated images:\")\n",
        "\n",
        "# Get a few simulated document images to test\n",
        "# Assuming 'documentos_img' directory exists from previous steps\n",
        "image_files_for_testing = sorted([f for f in os.listdir(\"documentos_img\") if f.endswith(\".png\")])[:5] # Test with the first 5 images\n",
        "\n",
        "# Check if necessary models and preprocessors are available (from previous steps)\n",
        "# We need the models trained on the combined OCR data\n",
        "required_vars = ['nn_combined_ocr', 'large_ocr_vectorizer', 'combined_ocr_encoder', 'preprocess_image_for_ocr', 'pytesseract', 'Image', 'limpiar_texto_ocr']\n",
        "all_available = True\n",
        "for var in required_vars:\n",
        "    if var not in globals() and var not in locals():\n",
        "        print(f\"Error: Required variable '{var}' not found.\")\n",
        "        all_available = False\n",
        "\n",
        "if all_available and image_files_for_testing:\n",
        "    # Use the models trained on the combined OCR data\n",
        "    current_vectorizer = large_ocr_vectorizer # Assuming large_ocr_vectorizer was updated in Step 7 (Revisited)\n",
        "    current_encoder = combined_ocr_encoder\n",
        "    current_nn = nn_combined_ocr\n",
        "\n",
        "\n",
        "    for image_file in image_files_for_testing:\n",
        "        image_path = os.path.join(\"documentos_img\", image_file)\n",
        "\n",
        "        # Call the prediction function\n",
        "        predicted_intent, extracted_cleaned_text = predict_intention_from_image(image_path, current_vectorizer, current_encoder, current_nn)\n",
        "\n",
        "        # Display the results\n",
        "        print(f\"\\nImage: {image_file}\")\n",
        "        print(f\"  Extracted & Cleaned Text: '{extracted_cleaned_text}'\")\n",
        "        print(f\"  Predicted Intent: {predicted_intent}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping testing: Required models/preprocessors not available or no test images found.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63effea0"
      },
      "source": [
        "## Combinar y decidir la respuesta\n",
        "\n",
        "### Subtask:\n",
        "Create a simple logic to determine a basic response based on the predicted banking intention. This logic should map each predicted intention to a predefined response string.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "238b33d2"
      },
      "source": [
        "# Define a mapping from predicted intention to a basic response\n",
        "intention_responses = {\n",
        "    \"Consultar saldo\": \"Claro, ¿para qué cuenta deseas consultar el saldo?\",\n",
        "    \"Transferir dinero\": \"Entendido, ¿cuánto y a quién deseas transferir dinero?\",\n",
        "    \"Pagar tarjeta de crédito\": \"Puedo ayudarte con el pago de tu tarjeta de crédito. ¿Cuál deseas pagar y cuánto?\",\n",
        "    \"Abrir cuenta\": \"Para abrir una nueva cuenta, necesito algunos datos. ¿Qué tipo de cuenta te interesa?\",\n",
        "    \"Cerrar cuenta\": \"Comprendo que deseas cerrar tu cuenta. ¿Podrías indicar el motivo?\",\n",
        "    \"Solicitar préstamo\": \"Puedo darte información sobre nuestros préstamos. ¿Qué tipo de préstamo te interesa (personal, hipotecario, etc.)?\",\n",
        "    \"Bloquear tarjeta\": \"Si tu tarjeta ha sido robada o perdida, puedo ayudarte a bloquearla inmediatamente. ¿Confirmas el bloqueo?\",\n",
        "    \"Actualizar datos personales\": \"Para actualizar tus datos personales, te guiaré a través del proceso de verificación. ¿Estás listo?\",\n",
        "    \"Consultar movimientos\": \"Claro, ¿de qué cuenta y período deseas consultar los movimientos?\",\n",
        "    \"Reportar transacción sospechosa\": \"Lamento escuchar eso. Por favor, describe la transacción sospechosa.\",\n",
        "    \"Cheque\": \"Detecté un cheque. ¿Necesitas procesarlo o alguna información sobre él?\",\n",
        "    \"Estado de cuenta\": \"Detecté un estado de cuenta. ¿Necesitas consultarlo o realizar alguna acción basada en él?\",\n",
        "    \"Recibo de depósito\": \"Detecté un recibo de depósito. ¿Quieres confirmar la transacción o necesitas más detalles?\",\n",
        "    \"Comprobante de transferencia\": \"Detecté un comprobante de transferencia. ¿Necesitas verificarla o tienes alguna pregunta?\",\n",
        "    # Add a default response for any unhandled intentions\n",
        "    \"Unknown Intent\": \"No estoy seguro de cómo ayudarte con eso. ¿Podrías reformular tu solicitud?\",\n",
        "    \"No meaningful text found after cleaning\": \"No pude extraer texto claro de la entrada. ¿Puedes intentarlo de nuevo con un texto más claro?\",\n",
        "    \"Prediction Error: Could not preprocess image.\": \"Hubo un error al procesar la imagen. Por favor, asegúrate de que el archivo de imagen sea válido.\",\n",
        "    \"Prediction Error during OCR extraction:\": \"Hubo un error al extraer texto de la imagen. El documento podría no ser legible.\",\n",
        "    \"Prediction Error: No meaningful text extracted or cleaned from image.\": \"No se pudo extraer texto significativo de la imagen. Por favor, intenta con un documento diferente.\",\n",
        "    \"Prediction Error during vectorization:\": \"Ocurrió un error al procesar el texto para la predicción.\",\n",
        "    \"Prediction Error during neural network prediction:\": \"Ocurrió un error al realizar la predicción con el modelo.\",\n",
        "    \"Prediction Error during decoding prediction:\": \"Ocurrió un error al interpretar la predicción.\",\n",
        "    \"Prediction Error: Could not decode prediction\": \"Ocurrió un error al decodificar la predicción.\",\n",
        "    \"Prediction Error: Text vectorizer model not found.\": \"Error: El modelo de vectorización de texto no está disponible.\",\n",
        "    \"Prediction Error: Text classification model not found.\": \"Error: El modelo de clasificación de texto no está disponible.\",\n",
        "    \"Prediction Error: Intention encoder not found.\": \"Error: El codificador de intenciones no está disponible.\",\n",
        "    \"Prediction Error: Text cleaning function 'limpiar_texto' not found.\": \"Error: La función de limpieza de texto no está disponible.\"\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "print(\"Defined intention_responses mapping.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74f32b14"
      },
      "source": [
        "## Implementar una función de entrada unificada\n",
        "\n",
        "### Subtask:\n",
        "Create a main function that can accept input (either text or image path) and a parameter indicating the input type ('text' or 'image'), and then route the processing to the appropriate function (`predict_intention_from_text` or `predict_intention_from_image_combined`) based on the input type. This function should return the predicted intention and the corresponding basic response.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f3a5e26"
      },
      "source": [
        "def process_input(input_data, input_type):\n",
        "    \"\"\"\n",
        "    Processes input (text or image) and predicts the banking intention, returning a basic response.\n",
        "\n",
        "    Args:\n",
        "        input_data (str): The input data, either direct text or the path to an image file.\n",
        "        input_type (str): The type of input, either 'text' or 'image'.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the predicted intention (str) and the corresponding response (str).\n",
        "               Returns error messages for invalid input types or processing failures.\n",
        "    \"\"\"\n",
        "    predicted_intention = \"Unknown Intent\" # Default intention\n",
        "\n",
        "    if input_type == 'text':\n",
        "        # Use the text prediction function\n",
        "        predicted_intention = predict_intention_from_text(input_data)\n",
        "        # If predict_intention_from_text returns an error string, use it as the intention\n",
        "        if predicted_intention.startswith(\"Prediction Error\") or predicted_intention.startswith(\"No meaningful text found\"):\n",
        "            response = intention_responses.get(predicted_intention, intention_responses[\"Unknown Intent\"])\n",
        "            return predicted_intention, response\n",
        "        extracted_text = input_data # For text input, extracted text is the input itself (or cleaned)\n",
        "\n",
        "    elif input_type == 'image':\n",
        "        # Ensure necessary models for image processing are available\n",
        "        if ('large_ocr_vectorizer' not in locals() and 'large_ocr_vectorizer' not in globals()) and \\\n",
        "           ('combined_ocr_vectorizer' not in locals() and 'combined_ocr_vectorizer' not in globals()):\n",
        "            return \"Prediction Error: OCR vectorizer model not found.\", intention_responses.get(\"Prediction Error: OCR vectorizer model not found.\", intention_responses[\"Unknown Intent\"])\n",
        "        if 'combined_ocr_encoder' not in locals() and 'combined_ocr_encoder' not in globals():\n",
        "            return \"Prediction Error: OCR encoder not found.\", intention_responses.get(\"Prediction Error: OCR encoder not found.\", intention_responses[\"Unknown Intent\"])\n",
        "        if 'nn_combined_ocr' not in locals() and 'nn_combined_ocr' not in globals():\n",
        "            return \"Prediction Error: Combined NN model not found.\", intention_responses.get(\"Prediction Error: Combined NN model not found.\", intention_responses[\"Unknown Intent\"])\n",
        "        if 'predict_intention_from_image' not in locals() and 'predict_intention_from_image' not in globals():\n",
        "             return \"Prediction Error: Image prediction function not found.\", intention_responses[\"Unknown Intent\"]\n",
        "\n",
        "\n",
        "        # Determine the correct vectorizer variable to use\n",
        "        current_ocr_vectorizer = large_ocr_vectorizer if 'large_ocr_vectorizer' in locals() or 'large_ocr_vectorizer' in globals() else combined_ocr_vectorizer\n",
        "\n",
        "\n",
        "        # Use the image prediction function\n",
        "        predicted_intention, extracted_text = predict_intention_from_image(input_data, current_ocr_vectorizer, combined_ocr_encoder, nn_combined_ocr)\n",
        "\n",
        "        # If predict_intention_from_image returns an error string as the intention, use it\n",
        "        if predicted_intention.startswith(\"Prediction Error\"):\n",
        "            response = intention_responses.get(predicted_intention, intention_responses[\"Unknown Intent\"])\n",
        "            return predicted_intention, response\n",
        "\n",
        "    else:\n",
        "        return \"Invalid input type specified.\", intention_responses[\"Unknown Intent\"]\n",
        "\n",
        "    # Look up the basic response based on the predicted intention\n",
        "    # Use the get() method with a default value to handle unseen intentions gracefully\n",
        "    response = intention_responses.get(predicted_intention, intention_responses[\"Unknown Intent\"])\n",
        "\n",
        "    return predicted_intention, response\n",
        "\n",
        "print(\"Defined process_input function.\")\n",
        "\n",
        "# === Optional: Test the process_input function ===\n",
        "print(\"\\nTesting the end-to-end process_input pipeline:\")\n",
        "\n",
        "# Test with text input\n",
        "test_text_input = \"Quiero saber mi saldo\"\n",
        "predicted_intent_text, response_text = process_input(test_text_input, 'text')\n",
        "print(f\"Input Type: Text, Input: '{test_text_input}'\")\n",
        "print(f\"  Predicted Intent: {predicted_intent_text}\")\n",
        "print(f\"  Response: {response_text}\")\n",
        "\n",
        "\n",
        "# Test with image input (use a sample image from the generated ones)\n",
        "# Assuming 'documentos_img' directory and image files exist from previous steps\n",
        "image_files_for_testing_process = sorted([f for f in os.listdir(\"documentos_img\") if f.endswith(\".png\")])\n",
        "if image_files_for_testing_process:\n",
        "    test_image_path = os.path.join(\"documentos_img\", image_files_for_testing_process[0])\n",
        "    predicted_intent_image, response_image = process_input(test_image_path, 'image')\n",
        "    print(f\"\\nInput Type: Image, Input: '{test_image_path}'\")\n",
        "    print(f\"  Predicted Intent: {predicted_intent_image}\")\n",
        "    print(f\"  Response: {response_image}\")\n",
        "else:\n",
        "    print(\"\\nNo image files found in 'documentos_img' for testing image input.\")\n",
        "\n",
        "\n",
        "# Test with invalid input type\n",
        "predicted_intent_invalid, response_invalid = process_input(\"some data\", 'video')\n",
        "print(f\"\\nInput Type: Invalid (video), Input: 'some data'\")\n",
        "print(f\"  Predicted Intent: {predicted_intent_invalid}\")\n",
        "print(f\"  Response: {response_invalid}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83f78a4b"
      },
      "source": [
        "## Probar la integración multimodal\n",
        "\n",
        "### Subtask:\n",
        "Test the main `process_input` function with examples of both text and image input to verify that both paths of the multimodal pipeline are working correctly and producing predicted intentions and responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab7d2a40"
      },
      "source": [
        "# === Test the main process_input function with examples ===\n",
        "print(\"--- Testing the End-to-End Multimodal Pipeline (process_input) ---\")\n",
        "\n",
        "# Test Case 1: Text Input\n",
        "test_text_input = \"Quiero consultar mi saldo\"\n",
        "print(f\"\\nTesting with Text Input: '{test_text_input}'\")\n",
        "predicted_intent_text, response_text = process_input(test_text_input, 'text')\n",
        "print(f\"  Predicted Intent: {predicted_intent_text}\")\n",
        "print(f\"  Response: {response_text}\")\n",
        "\n",
        "\n",
        "# Test Case 2: Image Input\n",
        "# Assuming 'documentos_img' directory and image files exist from previous steps\n",
        "image_files_for_testing_process = sorted([f for f in os.listdir(\"documentos_img\") if f.endswith(\".png\")])\n",
        "if image_files_for_testing_process:\n",
        "    test_image_path = os.path.join(\"documentos_img\", image_files_for_testing_process[0])\n",
        "    print(f\"\\nTesting with Image Input: '{test_image_path}'\")\n",
        "    predicted_intent_image, response_image = process_input(test_image_path, 'image')\n",
        "    print(f\"  Predicted Intent: {predicted_intent_image}\")\n",
        "    print(f\"  Response: {response_image}\")\n",
        "else:\n",
        "    print(\"\\nSkipping Image Input Test: No image files found in 'documentos_img'.\")\n",
        "\n",
        "\n",
        "# Test Case 3: Invalid Input Type\n",
        "test_invalid_input = \"some random data\"\n",
        "test_invalid_type = \"audio\"\n",
        "print(f\"\\nTesting with Invalid Input Type: '{test_invalid_type}'\")\n",
        "predicted_intent_invalid, response_invalid = process_input(test_invalid_input, test_invalid_type)\n",
        "print(f\"  Predicted Intent: {predicted_intent_invalid}\")\n",
        "print(f\"  Response: {response_invalid}\")\n",
        "\n",
        "print(\"\\n--- End of Multimodal Pipeline Tests ---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2fb5cd4"
      },
      "source": [
        "## Resumen de la Integración Multimodal\n",
        "\n",
        "### Hallazgos Clave del Análisis de Datos (y Pipeline Implementada)\n",
        "\n",
        "*   La pipeline diseñada puede aceptar dos tipos principales de entrada: **texto directo** e **imágenes de documentos**.\n",
        "*   Se implementó una función (`predict_intention_from_text`) para manejar la entrada de texto, utilizando el modelo de clasificación de texto previamente entrenado para predecir la intención bancaria.\n",
        "*   Se implementó una función (`predict_intention_from_image`) para manejar la entrada de imagen, que incorpora el preprocesamiento de la imagen, la realización de OCR para extraer texto, la limpieza del texto extraído y luego el uso del modelo de clasificación entrenado con datos de OCR para predecir la intención basándose en el texto de la imagen.\n",
        "*   Se creó un diccionario (`intention_responses`) para mapear las intenciones predichas (incluyendo tipos de documento como 'Cheque' y 'Estado de cuenta' que se derivan del OCR) y posibles mensajes de error a respuestas básicas predefinidas.\n",
        "*   Se creó una función unificada (`process_input`) como punto de entrada principal, que dirige la entrada a la función de predicción adecuada según el tipo de entrada especificado ('text' o 'image') y recupera la respuesta básica correspondiente.\n",
        "*   Las pruebas confirmaron que la función `process_input` maneja correctamente tanto las entradas de texto como las de imagen, demostrando ambos caminos de la pipeline multimodal. (Aunque se encontraron y resolvieron errores durante la implementación).\n",
        "*   El camino de entrada de imagen logró extraer texto, predecir la intención (por ejemplo, \"Recibo de depósito\" para una imagen de cheque de ejemplo) y proporcionar una respuesta.\n",
        "*   Se validó el manejo de errores para tipos de entrada no válidos en `process_input`.\n",
        "\n",
        "### Ideas o Próximos Pasos\n",
        "\n",
        "*   Mejorar la lógica de generación de respuestas para que sean más dinámicas y conscientes del contexto, basándose en la intención predicha y, potencialmente, en entidades extraídas del texto o la imagen de entrada.\n",
        "*   Expandir el módulo de visión para incluir la clasificación del *tipo* de documento (Cheque, Estado de Cuenta, etc.) utilizando técnicas de visión por computadora y entrenar un modelo específico para esta tarea.\n",
        "*   Considerar la extracción de *entidades* (como montos, fechas, nombres) del texto extraído por OCR para enriquecer la información disponible para la gestión del diálogo o la generación de respuestas.\n",
        "*   Desarrollar una gestión de diálogo más sofisticada que mantenga un estado y memoria conversacional para manejar interacciones de varios turnos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-34VnOrmtMc"
      },
      "source": [
        "# Clear Hugging Face cache directories\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Get the default cache directory\n",
        "# This might vary slightly based on environment, but this is common\n",
        "hf_cache_home = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"huggingface\")\n",
        "\n",
        "if os.path.exists(hf_cache_home):\n",
        "    print(f\"Clearing Hugging Face cache directory: {hf_cache_home}\")\n",
        "    try:\n",
        "        shutil.rmtree(hf_cache_home)\n",
        "        print(\"Cache cleared successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error clearing cache: {e}\")\n",
        "else:\n",
        "    print(f\"Hugging Face cache directory not found at {hf_cache_home}. No cache to clear.\")\n",
        "\n",
        "# Note: You might need to restart the runtime again after clearing the cache,\n",
        "# although clearing it programmatically often makes it effective immediately for new downloads.\n",
        "# It's safer to restart and then re-run the setup and model loading."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vbAFEQYRyAfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cc5b985"
      },
      "source": [
        "!pip install tf-keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ead0313"
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Módulo NLU con BERT"
      ],
      "metadata": {
        "id": "48F4Bbc0o-DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "import tensorflow as tf # Import TensorFlow\n",
        "\n",
        "modelo_name = \"bert-base-multilingual-cased\"\n",
        "num_clases = 3  # por ejemplo, número de etiquetas\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo_name)\n",
        "# from_pt=True indica que cargue los pesos desde un checkpoint de PyTorch si está disponible\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(modelo_name, num_labels=num_clases, from_pt=True)"
      ],
      "metadata": {
        "id": "Q8GgyJd9qdgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "modelo_name = \"bert-base-multilingual-cased\"  # Soporta español y otros idiomas\n",
        "\n",
        "# Define the number of classes based on your banking intentions\n",
        "intentos_o_clases = 10 # Assuming 10 classes based on the original dataset\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo_name)\n",
        "# Try to explicitly tell the library not to load from PyTorch checkpoint\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(modelo_name, num_labels=intentos_o_clases, from_pt=True)\n",
        "\n",
        "def predecir_intencion(texto):\n",
        "    inputs = tokenizer(texto, return_tensors=\"tf\", truncation=True, padding=True)\n",
        "    outputs = model(inputs)\n",
        "    logits = outputs.logits\n",
        "    pred = tf.argmax(logits, axis=1).numpy()[0]\n",
        "    return pred"
      ],
      "metadata": {
        "id": "X38-ztP3ji3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Módulo Visión\n",
        "OCR con pytesseract"
      ],
      "metadata": {
        "id": "ZVQ1XhmCo4Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "import cv2\n",
        "\n",
        "def extraer_texto(imagen_path):\n",
        "    img = cv2.imread(imagen_path)\n",
        "    gris = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    _, umbral = cv2.threshold(gris, 150, 255, cv2.THRESH_BINARY)\n",
        "    texto = pytesseract.image_to_string(umbral, lang='spa')\n",
        "    return texto\n"
      ],
      "metadata": {
        "id": "V34zZdNfoo8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clasificación de documentos"
      ],
      "metadata": {
        "id": "ceNmYoH8otkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "output = Dense(num_clases, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Congelar base_model si quieres solo entrenar la cabeza\n",
        "base_model.trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "mhAonxfToqPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gestión de diálogo"
      ],
      "metadata": {
        "id": "FnjoHuqwpDpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "chat_history_ids = None\n",
        "\n",
        "def responder_usuario(texto_usuario):\n",
        "    global chat_history_ids\n",
        "    new_input_ids = tokenizer.encode(texto_usuario + tokenizer.eos_token, return_tensors='pt')\n",
        "    bot_input_ids = new_input_ids if chat_history_ids is None else torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "    respuesta = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    return respuesta\n"
      ],
      "metadata": {
        "id": "slF8x8TtpH5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interfaz Web y API\n",
        "Con FastAPI (recomendado para API y web rápido):"
      ],
      "metadata": {
        "id": "qiym_bTCpMxP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5a10bc6"
      },
      "source": [
        "!pip install python-multipart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b90f170"
      },
      "source": [
        "!pip install fastapi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/procesar_documento/\")\n",
        "async def procesar_documento(file: UploadFile = File(...)):\n",
        "    # Guardar archivo temporalmente\n",
        "    contenido = await file.read()\n",
        "    with open(\"temp.png\", \"wb\") as f:\n",
        "        f.write(contenido)\n",
        "    texto = extraer_texto(\"temp.png\")  # usa función OCR antes definida\n",
        "    # Aquí agregas llamada a clasificador, NLU, diálogo, etc.\n",
        "    return JSONResponse({\"texto_extraido\": texto})\n",
        "\n",
        "# Ejecutar con: uvicorn nombre_script:app --reload\n"
      ],
      "metadata": {
        "id": "rFYL3zx7pQQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54a77720"
      },
      "source": [
        "## Instalar fastapi y uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfe3a57e"
      },
      "source": [
        "# Install FastAPI and Uvicorn\n",
        "!pip install fastapi uvicorn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fdffdb9"
      },
      "source": [
        "## Instalar y configurar ngrok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek6fRoWlBVYs"
      },
      "source": [
        "import time\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1. Configure your ngrok authentication token\n",
        "# *** IMPORTANT ***: Replace \"YOUR_NGROK_AUTH_TOKEN\" with your actual ngrok auth token.\n",
        "# You can get your token from your ngrok dashboard after signing up at ngrok.com\n",
        "NGROK_AUTH_TOKEN = \"30vg34wt2hnWqIxx7EyI4MxLfEk_tLv3MfwiJ4xwh9JqY1RS\" # <-- Replace this!\n",
        "\n",
        "if NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTH_TOKEN\":\n",
        "    print(\"Please replace 'YOUR_NGROK_AUTH_TOKEN' with your actual ngrok auth token.\")\n",
        "\n",
        "else:\n",
        "    try:\n",
        "        # Set the auth token\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "        print(\"ngrok authentication token set.\")\n",
        "\n",
        "        # 2. Define the port Uvicorn is running on (from previous step)\n",
        "        port = 8000 # Ensure this matches the port used by Uvicorn\n",
        "\n",
        "        # 3. Create an ngrok tunnel to the specified port\n",
        "        # Disconnect any existing tunnels first to avoid conflicts\n",
        "        ngrok.kill()\n",
        "        # Create a new http tunnel\n",
        "        # Use bind_tls=True for https tunnel as required by some browsers/APIs\n",
        "        public_url = ngrok.connect(port, bind_tls=True).public_url\n",
        "\n",
        "        # 4. Print the public URL provided by ngrok\n",
        "        if public_url:\n",
        "            print(f\"\\nFastAPI application is accessible at: {public_url}\")\n",
        "\n",
        "            # 5. Test the public URL using curl (GET request to root endpoint)\n",
        "            print(\"\\nTesting the public URL with curl...\")\n",
        "            # Use !curl to run shell command in Colab\n",
        "            # Note: For POST requests to /process_input/, you would need a more complex curl command or use requests library\n",
        "            !curl -X GET $public_url\n",
        "\n",
        "        else:\n",
        "            print(\"\\nFailed to create ngrok tunnel. The FastAPI application might not be publicly accessible.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during ngrok setup or testing: {e}\")\n",
        "\n",
        "    # 6. Keep the cell running to keep the server and tunnel active\n",
        "    # The Uvicorn server is assumed to be running in a separate thread from the previous cell.\n",
        "    # This cell needs to stay alive for the ngrok tunnel to persist.\n",
        "    print(\"\\nKeep this cell running to keep the ngrok tunnel active.\")\n",
        "    # Add a long sleep or similar mechanism if needed to prevent the cell from finishing prematurely\n",
        "    # time.sleep(3600) # Keep alive for 1 hour (Optional, depending on Colab's behavior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a855c24"
      },
      "source": [
        "from fastapi import FastAPI\n",
        "\n",
        "# Instantiate a FastAPI application\n",
        "app = FastAPI()\n",
        "\n",
        "# Define a root endpoint that responds to GET requests\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Hello from FastAPI in Colab!\"}\n",
        "\n",
        "print(\"FastAPI app instance created with a root endpoint.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c08acbb"
      },
      "source": [
        "from fastapi import FastAPI, UploadFile, File, Form\n",
        "from fastapi.responses import JSONResponse\n",
        "import os\n",
        "\n",
        "# Assuming the following functions/variables from previous steps are available:\n",
        "# - process_input(input_data, input_type)\n",
        "# - predict_intention_from_text(text)\n",
        "# - predict_intention_from_image(image_path, vectorizer, encoder, neural_network)\n",
        "# - intention_responses (dictionary)\n",
        "# - vectorizer (for text)\n",
        "# - nn (for text)\n",
        "# - encoder (for text)\n",
        "# - large_ocr_vectorizer or combined_ocr_vectorizer (for image)\n",
        "# - nn_combined_ocr (for image)\n",
        "# - combined_ocr_encoder (for image)\n",
        "# - limpiar_texto (for text cleaning)\n",
        "# - preprocess_image_for_ocr (for image preprocessing)\n",
        "# - pytesseract (for OCR)\n",
        "# - Image (from PIL)\n",
        "# - limpiar_texto_ocr (for OCR text cleaning)\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Define the root endpoint (optional, keep for basic check)\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"API is running. Use the /process_input/ endpoint for predictions.\"}\n",
        "\n",
        "# Define a new endpoint to handle multimodal input\n",
        "@app.post(\"/process_input/\")\n",
        "async def process_multimodal_input(\n",
        "    input_type: str = Form(...), # 'text' or 'image'\n",
        "    text_input: str = Form(None), # Optional text input\n",
        "    image_file: UploadFile = File(None) # Optional image file input\n",
        "):\n",
        "    \"\"\"\n",
        "    Processes either text or image input and predicts the banking intention.\n",
        "\n",
        "    Args:\n",
        "        input_type (str): Specifies the type of input ('text' or 'image').\n",
        "        text_input (str, optional): The direct text input if input_type is 'text'.\n",
        "        image_file (UploadFile, optional): The uploaded image file if input_type is 'image'.\n",
        "\n",
        "    Returns:\n",
        "        JSONResponse: A JSON object containing the predicted intention and response.\n",
        "    \"\"\"\n",
        "    predicted_intent = \"Unknown Intent\"\n",
        "    response = intention_responses.get(\"Unknown Intent\", \"No estoy seguro de cómo ayudarte con eso.\")\n",
        "    extracted_text = \"\" # To store extracted text from image if applicable\n",
        "\n",
        "    try:\n",
        "        if input_type == 'text':\n",
        "            if text_input is None:\n",
        "                return JSONResponse(status_code=400, content={\"error\": \"Text input is required for input_type 'text'.\"})\n",
        "\n",
        "            # Use the text prediction function\n",
        "            predicted_intent, response = process_input(text_input, 'text')\n",
        "            extracted_text = text_input # For text input, the \"extracted\" text is the input\n",
        "\n",
        "        elif input_type == 'image':\n",
        "            if image_file is None:\n",
        "                 return JSONResponse(status_code=400, content={\"error\": \"Image file is required for input_type 'image'.\"})\n",
        "\n",
        "            # Save the uploaded image temporarily\n",
        "            temp_image_path = \"temp_uploaded_image.png\"\n",
        "            with open(temp_image_path, \"wb\") as f:\n",
        "                f.write(await image_file.read())\n",
        "\n",
        "            # Use the image prediction function\n",
        "            # Ensure the correct vectorizer is used (large_ocr_vectorizer or combined_ocr_vectorizer)\n",
        "            current_ocr_vectorizer = large_ocr_vectorizer if 'large_ocr_vectorizer' in locals() or 'large_ocr_vectorizer' in globals() else (combined_ocr_vectorizer if 'combined_ocr_vectorizer' in locals() or 'combined_ocr_vectorizer' in globals() else None)\n",
        "\n",
        "            if current_ocr_vectorizer and 'combined_ocr_encoder' in globals() or 'combined_ocr_encoder' in locals() and 'nn_combined_ocr' in globals() or 'nn_combined_ocr' in locals():\n",
        "                 predicted_intent, extracted_text = predict_intention_from_image(temp_image_path, current_ocr_vectorizer, combined_ocr_encoder, nn_combined_ocr)\n",
        "            else:\n",
        "                 predicted_intent = \"Prediction Error: Required models for image processing not found.\"\n",
        "                 extracted_text = \"\"\n",
        "\n",
        "\n",
        "            # Clean up the temporary file\n",
        "            if os.path.exists(temp_image_path):\n",
        "                os.remove(temp_image_path)\n",
        "\n",
        "            # If the prediction function returned an error message as intent, use it for response\n",
        "            if predicted_intent.startswith(\"Prediction Error\"):\n",
        "                 response = intention_responses.get(predicted_intent, intention_responses[\"Unknown Intent\"])\n",
        "            else:\n",
        "                # Look up the response based on the predicted intention from image\n",
        "                response = intention_responses.get(predicted_intent, intention_responses[\"Unknown Intent\"])\n",
        "\n",
        "\n",
        "        else:\n",
        "            return JSONResponse(status_code=400, content={\"error\": \"Invalid input_type. Use 'text' or 'image'.\"})\n",
        "\n",
        "        # Return the predicted intention and response\n",
        "        return JSONResponse(content={\n",
        "            \"input_type\": input_type,\n",
        "            \"extracted_text\": extracted_text, # Include extracted text for context\n",
        "            \"predicted_intent\": predicted_intent,\n",
        "            \"response\": response\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected errors during processing\n",
        "        return JSONResponse(status_code=500, content={\"error\": f\"An internal error occurred: {e}\"})\n",
        "\n",
        "print(\"FastAPI endpoint /process_input/ defined and integrated with multimodal pipeline functions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z10xCj2NYiGg"
      },
      "source": [
        "import asyncio\n",
        "from uvicorn import Config, Server\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# 4. Define the host and port for the Uvicorn server\n",
        "host = \"0.0.0.0\"\n",
        "port = 8000\n",
        "\n",
        "# 5. Create an asyncio event loop\n",
        "loop = asyncio.get_event_loop()\n",
        "\n",
        "# 6. Create a uvicorn.Config object for the FastAPI app instance\n",
        "# Assuming the 'app' FastAPI instance is available from a previous cell\n",
        "config = Config(app=app, host=host, port=port, loop=loop)\n",
        "\n",
        "# 7. Create a uvicorn.Server instance with the created config\n",
        "server = Server(config=config)\n",
        "\n",
        "# Function to run the server in a separate thread\n",
        "def run_server():\n",
        "    loop.run_until_complete(server.serve())\n",
        "\n",
        "# 8. Start the Uvicorn server in a separate thread\n",
        "# Check if the loop is already running, if so, run the server directly\n",
        "# Otherwise, use a separate thread\n",
        "if loop.is_running():\n",
        "    print(\"Running server in the current loop.\")\n",
        "    # Use run_coroutine_threadsafe to submit the coroutine to the running loop\n",
        "    asyncio.run_coroutine_threadsafe(server.serve(), loop)\n",
        "else:\n",
        "    print(\"Running server in a new thread.\")\n",
        "    # Use a separate thread to run the loop and the server\n",
        "    server_thread = threading.Thread(target=run_server)\n",
        "    server_thread.start()\n",
        "\n",
        "# 9. Create an ngrok tunnel to the specified port\n",
        "try:\n",
        "    # Disconnect any existing tunnels first\n",
        "    ngrok.kill()\n",
        "    # Create a new http tunnel\n",
        "    public_url = ngrok.connect(port, bind_tls=True).public_url\n",
        "    # ngrok.connect returns a NgrokTunnel object, access the public_url attribute\n",
        "except Exception as e:\n",
        "    print(f\"Error creating ngrok tunnel: {e}\")\n",
        "    public_url = None\n",
        "\n",
        "# 10. Print the public URL provided by ngrok\n",
        "if public_url:\n",
        "    print(f\"\\nFastAPI application is accessible at: {public_url}\")\n",
        "else:\n",
        "    print(\"\\nFailed to create ngrok tunnel. The FastAPI application might not be publicly accessible.\")\n",
        "\n",
        "\n",
        "# 11. Keep the cell running to keep the server and tunnel active\n",
        "# You might need to add a placeholder like `time.sleep(3600)` in a real scenario\n",
        "# in interactive Colab to keep the cell running indefinitely, or rely on the\n",
        "# server thread keeping the process alive.\n",
        "# For demonstration, we'll print a message and the cell will remain active\n",
        "# until manually stopped or the Colab session ends.\n",
        "\n",
        "print(\"\\nKeep this cell running to keep the FastAPI server and ngrok tunnel active.\")\n",
        "\n",
        "# Optional: Add a long sleep if needed to prevent the cell from finishing prematurely\n",
        "# time.sleep(3600) # Keep alive for 1 hour"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "print(\"Intentando detener procesos ngrok existentes inmediatamente antes de iniciar el túnel...\")\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"Procesos ngrok detenidos.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al intentar detener procesos ngrok: {e}\")"
      ],
      "metadata": {
        "id": "sff9uj-EMisP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}